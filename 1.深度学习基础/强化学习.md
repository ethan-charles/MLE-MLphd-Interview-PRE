# 强化学习

## **1. 基本概念**

### **1.1 术语**

首先，我们来定义一些名词：

- agent(智能体)，可自由移动，对应玩家；
- action(动作)，由玩家做出，包括向上移动和出售物品等；
- reward(奖励)，由玩家获得，包括金币和杀死其他玩家等；
- environment(环境)，指玩家所处的地图或房间等；
- state(状态)，指玩家的当前状态，如位于地图中某个特定位置或房间中某个角落；
- goal(目标)，玩家的目标为获得尽可能多的奖励；

玩家(agent)将通过**与环境互动**并获得执行**行动**的**奖励**来学习环境。

> **agent** will learn from the **environment** by **interacting** with it and receiving **rewards** for performing **actions**.

这里“与环境互动”十分重要。其实，我们人类就是通过与环境互动来学习知识的。例如，你是一个襁褓中的孩子，你看到一个壁炉，并接近它。它很温暖，很积极，你感觉很好 *（积极奖励+1）。* 你感觉火是一件好事。

![img](https://pic1.zhimg.com/80/v2-c8ce8454742fa0737e827c619cb19f8c_1440w.jpg)

但是你试着触摸火焰，它会烧伤你的手 *（负奖励-1）*。你已经明白，当你离足够远时火是积极的，因为它会产生温暖。但是太靠近它会烧伤你。

![img](https://pic3.zhimg.com/80/v2-991a30f0f0564a6e02e9585ca6ddd06a_1440w.jpg)

这就是人类通过互动学习的方式。强化学习是一种从行动中与环境交互，从而学习的计算方法。

### **1.2 最大化预计累计奖励(expected cumulative reward)**

强化学习循环输出state，action和reward的序列，agent的目的是最大化预计累计奖励(expected cumulative reward)

每个时间步t的累积奖励可定义为：



![img](https://pic4.zhimg.com/80/v2-f603dd7fafe8fa8a8f6f19c0b68aee67_1440w.jpg)

即为t步之后所有步的收益之和，这相当于：

![img](https://pic4.zhimg.com/80/v2-d647b773bd0c6e5d350051c8af53d2db_1440w.jpg)

但是实际上，在早期提供的奖励更有用，因为它们比未来长期的奖励**更好预测**（predictable）。所以，我们按照时间对奖励进行**discount**. 折扣系数是一个位于0~1之间的数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 。

- ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)越大，折扣越小。这意味着agent更关心long-term reward。
- ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)越小，折扣越大。这意味着 agent 更关心short-term reward。

累积的折扣预期奖励(cumulative discounted reward at t)是：

![img](https://pic2.zhimg.com/80/v2-7798f7f1c0a0520736efe435e6a19221_1440w.jpg)

**1.3 Episodic or Continuing tasks**

- Episodic task: 有一个起点和终点（一个最终状态）。这会创建一个episode(就像电视剧的剧情一样)：状态States, 行动Actions, 奖励Rewards, 新状态 New States的list。例如，Super Mario游戏中，一个episode开始于游戏开始，结束于当马里奥被杀或者达到了关卡的末尾。
- Continuing tasks:没有终点状态。在这种情况下，agent必须学习如何选择最佳操作并同时与环境交互。例如，agent进行自动股票交易。对于此任务，没有起点和终点状态。agent 会持续执行，直到我们手动把它停止。

### **1.4 Exploration/Exploitation trade off**

- Exploration：寻找有关环境的更多信息。
- Exploitation：利用已知信息来最大化奖励。

## **2. Q-learning**

### **2.1 Q-table**

Q-table的列是不同的状态states，行是我们可选的actions，我们将计算每种状态 state 下采取的每种行动 action的**最大的未来预期奖励**(maximum expected feature rewards)。

> Each Q-table score will be the **maximum expected future reward** that I'll get if I **take that action** at **that state** with the **best policy given**.

Q-table就像一个cheatsheet，对于每个不同的state，我们都去查一查Q-table，然后找到在该state下使用不同的action带来的最大未来预期奖励，然后选择那个最大的action即可。

那么，如何去获得Q-table中每个元素的值呢？

### **2.2 Q-learning algorithm**

得到Q-table中每个元素值的函数叫Q-function, 它 有两个输入：“state”和“action”。它返回该action在该state下的预期未来奖励。

![img](https://pic1.zhimg.com/80/v2-ff9f3b8990b7d9d6847890396f0a1324_1440w.jpg)

**step1： 初始化Q-table**

我们将值初始化为0。

![img](https://pic1.zhimg.com/80/v2-6cce15608512319be6522e59ffeb456c_1440w.jpg)

之后，重复步骤2到4，直到算法运行次数为 episode 的最大值（由用户指定）或直到我们手动停止训练。

**step2. 选择**

据当前的Q-table，选择一个当前state下的action。但是......如果每个Q值都是零，那么该采取什么行动？

这就是我们谈到的**exploit/explore-tradeoff**的重要性。

![img](https://pic2.zhimg.com/80/v2-d5e11010884ed4760e3d4bcb1d4c0c45_1440w.jpg)

我们的想法是，在开始时，我们将使用**epsilon-greedy策略**：

- 我们指定一个探索率“epsilon”，我们在开始时设置为1，即**随机**执行的step的比例。刚开始学习时，这个速率必须是最高值，因为我们对Q-table的取值一无所知。这意味着我们需要通过随机选择我们的行动进行大量探索。
- 生成一个随机数。如果这个数字> epsilon，那么我们将进行“exploit”（这意味着我们使用已知的Q-table信息来选择每一步的**最佳**action）。否则，我们会进行explore。
- 在Q函数训练开始时我们必须有一个较大的epsilon。然后，随着Agent对于Q-table的值越来越确定，再减小这个epsilon。

**步骤3-4：评估**

采取action a 并观察结果state s' 和reward r，并更新Q-function Q(s,a)。

我们采取我们在步骤2中选择的action，然后执行此action将返回一个新的state s'和reward r。

然后，使用Bellman方程更新Q(s,a)：

![img](https://pic3.zhimg.com/80/v2-6f0c259e9e7a662d93007f5b74d6f21a_1440w.jpg)

```python
New Q value =   Current Q value +   lr * [Reward + discount_rate * (highest Q value between possible actions from the new state s’ ) — Current Q value ]
```



------

示例：

用Q-learning玩FrozenLake游戏：

![img](https://pic3.zhimg.com/80/v2-9b754e941a17b38b569c53b0112f6f56_1440w.jpg)

从start点走到end点，只能走冰面而不能走漩涡；但是，由于冰面很滑，所以并不一定每次都能走到你想走的位置。

首先，初始化Q-table：

```text
env = gym.make("FrozenLake-v0")
action_size = env.action_space.n
state_size = env.observation_space.n
qtable = np.zeros((state_size, action_size))
print(qtable)
```

![img](https://pic3.zhimg.com/80/v2-091adcc737b75d399d6359b6dfe3abde_1440w.jpg)

```python
##parameters
total_episodes = 15000        # Total episodes
learning_rate = 0.8           # Learning rate
max_steps = 99                # Max steps per episode
gamma = 0.95                  # Discounting rate

# Exploration parameters
epsilon = 1.0                 # Exploration rate
max_epsilon = 1.0             # Exploration probability at start
min_epsilon = 0.01            # Minimum exploration probability 
decay_rate = 0.005             # Exponential decay rate for exploration prob
```

Now we implement the Q learning algorithm:

```python
# List of rewards
rewards = []

# 2 For life or until learning is stopped
for episode in range(total_episodes):
    # Reset the environment
    state = env.reset()
    step = 0
    done = False
    total_rewards = 0
    
    for step in range(max_steps):
        # 3. Choose an action a in the current world state (s)
        ## First we randomize a number
        exp_exp_tradeoff = random.uniform(0, 1)
        
        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)
        if exp_exp_tradeoff > epsilon:
            action = np.argmax(qtable[state,:])

        # Else doing a random choice --> exploration
        else:
            action = env.action_space.sample()

        # Take the action (a) and observe the outcome state(s') and reward (r)
        new_state, reward, done, info = env.step(action)

        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
        # qtable[new_state,:] : all the actions we can take from new state
        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])
        
        total_rewards += reward
         # Our new state is state
        state = new_state
        
        # If done (if we're dead) : finish episode
        if done == True: 
            break
        
    # Reduce epsilon (because we need less and less exploration)
    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) 
    rewards.append(total_rewards)

print ("Score over time: " +  str(sum(rewards)/total_episodes))
print(qtable)
```



![img](https://pic3.zhimg.com/80/v2-8e1de66fc36861a208a30afe825eec4e_1440w.jpg)

After 10 000 episodes, our Q-table can be used as a "cheatsheet" to play FrozenLake"!

-----



Q-Learning生成一个Q-table，Agent用它来查找给定state时采取的最佳action。但正如我们所看到的，状态空间特别大的时候，生成和更新Q-table会非常的效率低下。所以，我们可以通过一个神经网络实现Q-table的功能，该神经网络获取agent的state并为该state的每个action计算Q值。

![img](https://pic4.zhimg.com/80/v2-97fbc41f5ad41e14d71f3668f7dfe9d7_1440w.jpg)

### 1. Deep Q-learning结构

先上整体的架构图，之后我们来逐步地解释：

![img](https://pic2.zhimg.com/80/v2-2849945a673eb8cc88302e266df170f5_1440w.jpg)

我们的Deep Q-learning网络以四个图像帧的stacking作为输入。它们通过这个网络，并在给定state下为每个可能的action输出Q值。我们需要采用最大Q值来找到我们最好的action。

一开始，智能体的表现非常糟糕。但随着时间的推移，它开始将 图像帧（state）与最佳action联系起来。

**1.1 预处理部分**

![img](https://pic4.zhimg.com/80/v2-91fe0a2271bbdccbf2c1c5d52c85695b_1440w.jpg)

我们希望降低state(frame)的复杂度，以减少训练所需的计算时间。

因此，首先我们可以对每个state进行灰度化。这是因为颜色在这个例子中不是一个重要信息（我们只需要找到敌人并杀死他，而不需要颜色来找到他）。所以可以将三种颜色通道（RGB）减少到1（灰度）。

然后，我们裁剪图像。在我们的例子中，看到屋顶没什么意义。

然后我们减小每帧图的大小，并将四个子帧叠加在一起。将帧堆叠在一起，是因为它有助于我们处理时间限制（temporal limitation）的问题。时间序列在很多游戏中很重要，例如一个乒乓球游戏，当你看到这个图片时：

![img](https://pic1.zhimg.com/80/v2-deb1409009354aca4ad828e42d0f017c_1440w.jpg)

我们并不能知道中间的球在往哪边移动，因为一帧图片不足以产生运动感！而通过下面这三帧图片，你可以看出球在向右移动，还可以知道它移动的速度：

![img](https://pic2.zhimg.com/80/v2-8c06e28d62858ebebdc5a1b73064000d_1440w.jpg)

**1.2 卷积网络**

用三个卷积层来处理帧，利用输入的空间、时序信息。之后通过全连接层得到每个action的Q值估计。

**经验回放(experience replay)：making more efficient use of observed experience**

经验回放将帮助我们处理两件事：

1）避免忘记以前的经历。Avoid forgetting previous experiences.

我们有一个很大的问题：权重的可变性，因为action和state之间存在高度相关性。

![img](https://pic4.zhimg.com/80/v2-ada10417a2ca76cfacaca7d4830b5ffb_1440w.jpg)

在每个time step,我们都得到元组 (state, action, reward, new_state). 我们把这个tuple喂进神经网络进行学习，然后就把它给丢掉了。我们每次都按照**时间顺序**去把和环境交互得到的元组喂进神经网络，这样就忘记了之前的experience。（这就是训练神经网络中重要的“打散”问题，不能让后面训练的sample把前面都覆盖了）。

解决方案：创建一个“replay buffer”。在agent与环境交互时存储experience tuple，然后我们用小批量数据 (small batch of tuple)来训练神经网络。

![img](https://pic1.zhimg.com/80/v2-46f8013aa1db0f887cff1fe718c4ecc8_1440w.jpg)

“replay buffer”可以看成一个文件夹，其中每个数据都是experience tuple，是通过agent与环境交互来产生的。然后拿其中的一些随机数据来训练神经网络，这可以防止网络只学习智能体当前的经验。

2）减少经验之间的相关性。Reduce correlations between experiences.

我们知道每个action都会影响下一个state。如果按序列顺序训练网络，那么我们得到的序列元组可能会高度相关。

通过在replay buffer随机抽取，我们可以打破这种相关性。可以防止动作值发生振荡或发散。

整个算法如下：

![img](https://pic4.zhimg.com/80/v2-7e4ca671464d19ec8d8e93f5fab4256b_1440w.jpg)

注意这里有个细节的优化，那就是我们要让function Q去学习的target ![[公式]](https://www.zhihu.com/equation?tex=y_i) 在这里是用另一个网络 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D) 来建模的。如果我们还用 ![[公式]](https://www.zhihu.com/equation?tex=Q) 来建模target的话，就会出现“我们不断更新Q使之接近target，但是target还随着Q的变化而变化”这样诡异的局面。我们每隔C步再去让 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D+%3D+Q) , 这样至少在这C步中，我们要学习的target都是固定的，这样可以让网络更加稳定。

这个优化叫做"fixed Q-target"。

**Prioritized Experience Replay**
某些experience可能比其他experience更重要，但它们出现的频率更低。如果我们只是对batch进行随机抽样，这类样本几乎没有机会被选中。所以，在Prioritized Experience Replay中，我们通过使用一个标准来定义每个experience元组的优先级(priority)来改变采样分布。
当预测值 ![[公式]](https://www.zhihu.com/equation?tex=Q%28%5Cphi_j%2C+a_j%2C+%5Ctheta%29) 和目标 ![[公式]](https://www.zhihu.com/equation?tex=y_i) 存在很大差异时，意味着模型需要多了解该experience的信息，因此优先级 ![[公式]](https://www.zhihu.com/equation?tex=p_t) 应该更高。
所以，我们把“优先级”这个信息也放入replay buffer中：

![img](https://pic3.zhimg.com/80/v2-bd36b55c8954b27d5631f8e7e4860c96_1440w.jpg)

但是不能只贪婪的选择优先级最高的那些experience，因为这会导致总是训练相同的experience。因此，引入随机优先级的概念，产生一个样本被选择中用于replay的概率。



![img](https://pic4.zhimg.com/80/v2-6dbc0b66fee00dbf340a7f9550b59bb7_1440w.jpg)