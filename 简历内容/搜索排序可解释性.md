“开发用户搜索界面排序算法的可解释性。实现了基于交叉特征检测方法NID和模型无关局部可解释性算法LIME的全局交叉特征检测算法GLIDER, 并在现有模型上加入重要的二阶交叉特征，在线上实现了0.4%的CTR提升。”



GLIDER: Global Interaction Detection and Encoding for Recommendation (GLIDER)

NID: Neural Interaction Detection 

LIME: data-instance perturbation method



为什么想到要发现重要的**交叉特征**呢？这是因为如果我们想要用广义加法模型来获得更好的可解释性，就必定少了自动特征交叉，所以要手动来构造交叉特征。



现有的解释性方法大多着重于**单个特征重要度**的解释。本文介绍的是一种发现神经网络学习到的**任意阶的交叉特征**的方法，该方法通过贪心策略搜索特征的组合和对神经元重要度的近似计算等方式来降低计算复杂度。主要包含以下两篇论文：

1. Detecting Statistical Interactions from Neural Network Weights (ICLR'18)
2. Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection (ICLR'19)

第一篇文章提出一种**交叉特征检测的方法 (NID)**，主要是发现 **MLP** 学习到的比较重要的交叉特征。第二篇文章把 NID 方法用到推荐模型上**，去发现推荐模型学习到的交叉特征**。此外，**它还把发现的交叉特征加到训练集上，然后重新训练模型提升模型效果。**("both interpret and augment the predictions of black-box recommender systems.")



#### Neural Interaction Detection (NID)

“detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights”

为了找到重要的交叉特征，我们首先要想一件事情：怎样来衡量交叉特征的重要度呢？更基本的，怎样来衡量一个神经元的重要度呢？

第 $l$ 层的第 $i$ 个神经元重要程度为 $z_i^{(l)}$, 它表是神经元 ![[公式]](https://www.zhihu.com/equation?tex=i) 对**最终预测** ![[公式]](https://www.zhihu.com/equation?tex=y) 的影响，或者说是神经元 ![[公式]](https://www.zhihu.com/equation?tex=i) 的重要度。它可以用一个权重矩阵连乘来表示：

![[公式]](https://www.zhihu.com/equation?tex=%5CLarge+z%5E%7B%281%29%7D+%3D+%7B%7C%5Cmathbf%7Bw%7D%5Ey%7C%7D%5ET%5Ccdot%5Cprod_%7Bl%3DL%7D%5E%7B2%7D%7C%5Cmathbf%7Bw%7D%5E%7B%28l%29%7D%7C+%5C%5C+%5CLarge+z_i%5E%7B%281%29%7D%5Cgeq+%7C%5Cfrac%7B%5Cpartial+y%7D%7B%5Cpartial+h_i%5E%7B%281%29%7D%7D+%7C%5C%5C)

它是模型输出 ![[公式]](https://www.zhihu.com/equation?tex=y) 对神经元 ![[公式]](https://www.zhihu.com/equation?tex=i) 输出**梯度绝对值的上界**。这个不等式的证明参考原文的附录C. 为啥要证明它是梯度绝对值的上界，是因为**梯度绝对值是一种常见的重要度度量方案**，这里NID**用权重矩阵连乘来近似**。



好了，现在我们知道一个神经元的重要程度如何度量了，那么现在该研究一下交叉特征的重要程度该如何度量了。对于一组交叉特征I，我们考察第二层（也就是第一个隐层）每一个节点。在第一个隐层的第 ![[公式]](https://www.zhihu.com/equation?tex=i) 个神经元上，交叉特征 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BI%7D) 的强度(interaction strength)记为![[公式]](https://www.zhihu.com/equation?tex=%5Comega_i%28%5Cmathcal%7BI%7D%29)。整个模型交叉特征 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BI%7D) 的组合强度则是把 ![[公式]](https://www.zhihu.com/equation?tex=%5Comega_i%28%5Cmathcal%7BI%7D%29) 累加起来，记为 ![[公式]](https://www.zhihu.com/equation?tex=%5Comega%28%5Cmathcal%7BI%7D%29)

那么，第i个神经元的交叉特征强度怎么计算呢？从下面式子可以看出，这个交叉特征的强度是神经元 ![[公式]](https://www.zhihu.com/equation?tex=i) 前面部分( ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%28%7C%5Cmathbf%7Bw%7D_%7Bi%2C%5Cmathcal%7BI%7D%7D%5E%7B%281%29%7D%7C%29) )和后面部分( ![[公式]](https://www.zhihu.com/equation?tex=z_i%5E%7B%281%29%7D) )的乘积，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%28%7C%5Cmathbf%7Bw%7D_%7Bi%2C%5Cmathcal%7BI%7D%7D%5E%7B%281%29%7D%7C%29)是交叉特征 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BI%7D) 与神经元 ![[公式]](https://www.zhihu.com/equation?tex=i) 连接的权重![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D_%7B%5Cmathcal%7BI%7D%7D) 的某种均值函数（实际取的是min）,

![[公式]](https://www.zhihu.com/equation?tex=%5CLarge%5Cbegin%7Bsplit%7D%5Comega%28%5Cmathcal%7BI%7D%29+%26%3D+%5Csum_%7Bi%3D1%7D%5E%7Bp_1%7D%5Comega_i%28%5Cmathcal%7BI%7D%29%5C%5C+%5Comega_i%28%5Cmathcal%7BI%7D%29+%26%3D+z_i%5E%7B%281%29%7D%5Ccdot%5Cmu%28%7C%5Cmathbf%7Bw%7D_%7Bi%2C%5Cmathcal%7BI%7D%7D%5E%7B%281%29%7D%7C%29%5Cend%7Bsplit%7D+%5C%5C)



![img](https://pic4.zhimg.com/80/v2-b6cc2f239153fa042a879dbcd105b20f_1440w.png)图



### 应用到推荐算法

用 NID 去发现推荐模型学习到的交叉特征，然后根据这些特征组合显式地在数据集![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D)上去产生交叉特征，生成新一个新的数据集![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%5E%7B%27%7D)，然后用这个新的数据集训练模型。这个过程中涉及到三个模型，第一个是被提取交叉特征的模型我们称之为 **source model**，第二个是NID 提取交叉特征用到的**MLP**，第三个是在生成的数据集上重新训练的模型称之为 **target model**。这个过程可以看成知识的迁移，从 source model 到 target model。文章提到**如果 source model 比 target model复杂，这个过程可以看成是模型的蒸馏（distillation），如果 source model 和 target model 是同一个模型，则这个过程可以看成模型增强（enhancement）**。

NID只能发现 **MLP 的交叉特征**，第二篇文章结合 **LIME** 方法学习一个**局部代理模型(MLP)**，然后再使用 NID 去发现这个代理模型的交叉特征。

这里简单说下 LIME 扰动数据的思路：给一个样本![[公式]](https://www.zhihu.com/equation?tex=x)，我们可以**随机改变它的某一维特征值**，对于**实值类型则置为默认值（例如0）**，这样就能得到一个**新样本**![[公式]](https://www.zhihu.com/equation?tex=x%27)，重复 n 次就能根据一个样本**生成 n 个样本**。这些样本都是分布在原始样本的**附近**，那么可以用分类模型![[公式]](https://www.zhihu.com/equation?tex=f_%7Brec%7D)对这些样本进行预测，这样就能构成一个新的数据集![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D_%7Bp%7D%3D%5C%7B%3Cx%5E%7B%27%7D%2Cy%5E%7B%27%7D%3Df_%7Brec%7D%28x%5E%7B%27%7D%29%3E%7Cx%5E%7B%27%7D%5Cin+%5Ctext%7Bperturbate%7D%28x%29%5C%7D)，这个生成的过程参考下图。 然后在这个新的数据集上的训练一个MLP，并用**NID去检测这个 MLP 的交叉特征**。到目前为止得到的交叉特征可以看作是 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Brec%7D) 的**局部**代理模型（毕竟 MLP 是在 x **附近**的点上训练得到的）。第二篇文章中提出针对推荐模型的**全局**特征检测方法：随机选取![[公式]](https://www.zhihu.com/equation?tex=N)个点，重复上面用代理模型做交叉特征检测的操作，然后把这 N 次结果得到的特征组合累计。

![img](https://pic4.zhimg.com/80/v2-4059606c689483e8d52df0d3ead29eaf_1440w.jpg)



### 实验

#### NID-实验设置&结果

第一篇的文章的实验对象是 MLP，激活函数是 ReLU。实验数据主要包含真实数据集和一部分由解析式已知的函数**合成**的数据集。下图是合成数据集生成的公式，以及对应的结果，红色的叉是表示两个特征存在交叉，蓝色方块表示模型学习到的交叉特征。（这里我们只列出合成数据集的实验结果）

![img](https://pic4.zhimg.com/80/v2-fd6f8a60610a424b9866ede500dcb5af_1440w.jpg)👆 合成数据的所用函数的表达式

![img](https://pic1.zhimg.com/80/v2-8a454d906e47f9e4d07169de766a6894_1440w.jpg)👆合成数据pairwise 交叉的结果，可见，的确模型学习到了真实的特征交叉。



#### GLIDER-实验设置&结果

扰动数据集包含5000训练样本500测试样本，检测特征交叉的 MLP 网络结构是![[公式]](https://www.zhihu.com/equation?tex=256%5Ctimes+128%5Ctimes+64), batch size=100，epoch=100, optimizer=adam, learning rare=0.01, l1-norm=![[公式]](https://www.zhihu.com/equation?tex=1e-4)。 NID 选择的激活函数使用的是 ReLU。数据集选择的criteo 和 avazu。实验结果见下图 表格里的 “+GLIDER” 是使用交叉特征重新训练后的结果。

![img](https://pic3.zhimg.com/80/v2-3518b3e08a4fdebeac2dcab2e627ccd2_1440w.jpg)👆GLIDER 实验结果，可见，根据GLIDER发现重要的交叉特征，然后显式加入原模型，可以起到模型增强的效果。

## 总结

文章提出一种提取模型学习到的**任意阶特征交叉**的方法，同时把提取到的交叉特征添加到数据集中然后重训模型提升性能。从实验结果上来看，在合成数据集上能发现一些特征组合，但是在真实数据上的性能提升还比较有限。