# 介绍FISTA-Net

FISTA-Net is composed of several key modules that work together for enhanced image reconstruction:

1. **Gradient Descent Module**: This layer performs gradient descent on the data fidelity term, enforcing consistency with observed data.
2. **CNN-based Proximal Mapping**: Acting as a learned proximal operator, this module improves image quality by applying learned regularization, replacing traditional soft-thresholding.
3. **Iterative Update Mechanism**: This component iterates updates using FISTA’s momentum, speeding convergence by balancing fidelity and regularization.

Each iteration improves the reconstruction by combining these modules, resulting in high-quality outputs suitable for inverse problems in imaging.

---

## FISTA和Iterative Shrinkage/Thresholding Algorithm (ISTA)的区别

**FISTA (Fast Iterative Shrinkage-Thresholding Algorithm)** improves upon **ISTA (Iterative Shrinkage-Thresholding Algorithm)** by adding a momentum term that accelerates convergence. While ISTA only applies simple iterative updates with soft-thresholding, FISTA introduces a "momentum" or extrapolation term, which helps the solution reach convergence faster, especially for large-scale or high-dimensional problems. This acceleration makes FISTA especially advantageous when dealing with complex inverse problems in imaging or other computationally intense applications.

---

## Momentum 在优化算法中的作用

"Momentum" 在优化算法中是一种技术，用于加速收敛速度。它通过结合前一步更新的方向和幅度，来影响当前的更新，平滑迭代过程，减少震荡现象。在 **FISTA** 中，momentum 用于“外推”每一步的更新，从而比普通的 ISTA 更快收敛，这在高维或复杂问题中尤其有效。

---

## CNN的作用

In **FISTA-Net**, the convolutional neural network (**CNN**) component is used as a learned proximal operator. It plays a critical role in refining image quality by effectively mapping noisy or incomplete reconstructions to cleaner images. This CNN-based proximal mapping helps in regularizing and enhancing the reconstruction at each iteration, which leads to better visual fidelity and accuracy in the final output, especially in challenging imaging scenarios such as low-resolution or noisy data.

---

## Soft Thresholding 的作用

**Soft thresholding** is effective in image processing, particularly for denoising and sparse representation, due to the following reasons:

1. **Smooth Noise Reduction**: Soft thresholding reduces small noise values towards zero without completely removing them, resulting in smoother, more natural images than hard thresholding.
2. **Sparse Representation**: It preserves large coefficients (significant image features) while compressing smaller ones (noise), making it ideal for retaining important image details while suppressing noise.
3. **Balance Between Detail and Noise Suppression**: Adjustable thresholds allow soft thresholding to balance noise reduction and detail preservation, crucial for natural images.
4. **Multi-Scale Adaptability**: By applying soft thresholding across different scales (e.g., wavelet decomposition), it effectively denoises both fine details and larger structures.

---

## FISTA-Net 对 FISTA 的提升

**FISTA-Net** builds on **FISTA** by integrating a CNN-based learned proximal mapping rather than using a fixed thresholding operator. This modification allows FISTA-Net to adapt more flexibly to various data characteristics, effectively regularizing complex patterns and noise structures within images. The CNN enhances the model’s ability to recover high-quality reconstructions across different noise levels and resolutions, improving upon FISTA's speed and generalization while retaining its accelerated convergence through the momentum term.

---

## FISTA-Net 主要的公式

The proposed **FISTA-Net** to solve is formulated as:

1. \( r^{(k)} = y^{(k)} - W^{(k)^T} A y^{(k)} - b \)  
2. \( x^{(k)} = T^{(k)} (r^{(k)}) \)  
3. \( y^{(k+1)} = x^{(k)} + \eta^{(k)} (x^{(k)} - x^{(k-1)}) \)

---

## 空间分辨率的测量：边缘锐利度

**边缘锐利度**是指图像边缘的清晰度。高分辨率图像的边缘应该是锐利的，而不是模糊或“拖尾”的。边缘锐利度可以反映重建算法的分辨能力，因为它揭示了算法在重建过程中是否丢失了原始数据中的边界信息。

一种常用的边缘锐利度测量方法是计算边缘梯度。可以通过梯度算子（如 **Sobel 算子**）来计算边缘的梯度强度，梯度越高，边缘越锐利，从而表明空间分辨率越高。此外，边缘轮廓的宽度（例如通过 **半最大高宽度，Full Width at Half Maximum (FWHM)**）也可以用来判断边缘锐利度——边缘越窄，表示分辨率越高。

---

## ReLU 在 FISTA 中的作用

**ReLU** 是一种简单而有效的非线性激活函数，能够快速增加网络的非线性表达能力，同时避免了梯度消失问题。这样可以在 **FISTA（快速迭代收缩阈值算法）** 的迭代过程中，使模型更快地收敛到理想的解。

---

## FISTA-NET 的核心任务：多视图数据的空间特征提取

**FISTA-NET** 的核心任务是从多视图数据中提取空间特征。**Conv3D（三维卷积）** 适合用来处理这种多视图三维数据，因为它可以在三维空间中提取特征，能够捕捉图像在三个维度（例如深度、高度和宽度）上的局部结构。

---

## 不建议加入 Pooling 操作的原因

1. **可能导致空间分辨率损失**：通过减少特征图的空间维度，可以降低计算量，但也可能导致图像细节信息的丢失。
2. **高分辨率需求**：FISTA-NET 的目标是精细地重建图像，并保留高分辨率的细节。引入 **Pooling** 可能导致细小特征（如边缘和纹理）被模糊化，降低了重建质量。

---

## MedSegDiff-V2 基本原理

**MedSegDiff-V2** is a diffusion-based model for medical image segmentation that combines **Vision Transformers** with diffusion probabilistic modeling to enhance segmentation precision. Its architecture introduces an **Anchor Condition** to stabilize predictions and a **Semantic Condition** via a **Spectrum-Space Transformer (SS-Former)** for better feature alignment across modalities. This structure uniquely enables it to handle diverse and noisy medical imaging data, achieving significant improvements over traditional models by capturing finer details and aligning features effectively.

---

## The Swin Transformer 基本原理

The **Swin Transformer**, or **Shifted Window Transformer**, is a variant of the Vision Transformer designed for hierarchical feature extraction, particularly useful in visual tasks. Its core innovation lies in the **shifted window mechanism**, which processes smaller, overlapping patches within images, enhancing local feature capture and long-range dependency modeling. This structure enables Swin to handle high-resolution inputs efficiently while maintaining computational efficiency. The Swin architecture's adaptability and hierarchical structure make it unique, positioning it as a powerful backbone for segmentation, detection, and recognition tasks.

---

## Swin Transformer 和 MedSegDiff-V2 的区别

- **Swin Transformer**: Uses a hierarchical, shifted window-based self-attention mechanism to extract multi-scale features with local-to-global receptive fields, ideal for general image tasks.
- **MedSegDiff-V2**: Leverages diffusion probabilistic modeling alongside Vision Transformers for feature alignment and noise handling, tailored specifically for robust medical image segmentation.

---

## 模型结构对比

### UNet

- **结构**: **UNet** is an encoder-decoder architecture. The encoder gradually reduces the image size to extract high-level features, while the decoder progressively restores the size to reconstruct the image.
- **跳跃连接 (Skip Connections)**: A distinctive feature of UNet is the use of skip connections between the encoder and decoder, which helps retain details during reconstruction by passing high-resolution features to the decoder.
- **层级结构**: Each layer consists of convolution and down-sampling, capturing progressively higher-level features.

### Stable Diffusion

- **结构**: **Stable Diffusion** consists of multiple cascaded U-Net models used for the diffusion and denoising processes.
- **扩散过程 (Diffusion Process)**: Adds Gaussian noise to the image iteratively, progressively degrading the image to pure noise.
- **反扩散过程 (Denoising Process)**: Gradually removes noise through a conditional sampling process, generating a high-quality image from the noise.

---

## 数学原理对比

### UNet

- **卷积操作**: Suppose the input image is \( x \), convolution with kernel \( W \) and bias \( b \) is represented as \( f(x) = W * x + b \).
- **损失函数**: Typically uses cross-entropy or Dice coefficient to optimize pixel-level prediction accuracy in image segmentation.

### Stable Diffusion

- **扩散方程**: According to the forward process of a Markov chain, noise is recursively added to the image, represented by: 
  \( q(x_t | x_{t-1}) = N(x_t; (1 - \beta_t) \cdot x_{t-1}, \beta_t \cdot I) \), where \( \beta_t \) controls the noise intensity.
- **逆向过程**: In the reverse diffusion process, noise is iteratively removed using conditional probabilities, with the goal of predicting noise terms using neural network parameters \( \theta \):
  \( p_\theta(x_{t-1} | x_t) = N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) \).
- **损失函数**: Commonly uses a KL divergence-based loss function to minimize the difference between the noise generated by the model and the true noise distribution.

---

## 四种食物图像分析方法

1. **Similar Image Search**: This traditional approach calculates similarity between an uploaded food image and images in a database, using models like **VGG-16** and **ResNet-50** for feature extraction.

2. **Improved Similar Image Search**: This optimized version pre-processes the database to enhance retrieval speed and accuracy using vector embeddings and similarity measures such as **Cosine Similarity**.

3. **Image to Ingredient to Recipe**: This approach predicts ingredients from the food image first, using models like **Transformers**, then generates a recipe based on detected ingredients.

4. **Inverse Cooking**: This method identifies ingredients in an image and uses them to generate a recipe, utilizing **Transformers with attention mechanisms** to integrate ingredient and image information for better recipe generation.

---

## 逆向烹饪（Inverse Cooking）基本原理

**Inverse Cooking** uses a **Transformer** architecture to generate recipes by capturing multimodal relationships between images and ingredients, enhancing recipe generation accuracy. This method primarily includes two phases:

### 图像与成分编码 (Image and Ingredient Encoding)

1. The model pre-trains an **image encoder** and **ingredient decoder** to extract visual features from the input image and predict the ingredients.
2. The encoder uses a convolutional neural network (e.g., **ResNet**) to encode the image into high-dimensional vector representations, capturing the visual characteristics of the food.
3. Simultaneously, the ingredient decoder encodes the ingredients into vectors, forming the basis for recipe generation.

### 基于 Transformer 的配方生成 (Recipe Generation with Transformer)

1. In the recipe generation phase, the model uses a **Transformer** instead of a traditional RNN. Each Transformer block consists of two parts: **Self-Attention Layer** and **Conditional Attention Layer**.
2. **Self-Attention Layer**: This is the classic self-attention mechanism in the Transformer, used to capture contextual information during recipe generation, i.e., the relationship between recipe steps.
3. **Conditional Attention Layer**: This layer is adapted to handle **multimodal attention**, combining image features and ingredient embedding information. Specifically, after self-attention, this layer incorporates embeddings from the image and ingredients to refine recipe generation, ensuring consistency between the generated steps and ingredients.
4. During experiments, researchers tried various attention fusion structures (e.g., parallel, additive), and found that directly **concatenating** image features and ingredient embeddings yielded the best results.

Through this multimodal Transformer architecture, the **Inverse Cooking** method can accurately generate recipe steps based on the image, significantly improving the diversity and accuracy of the generated recipes.
