
## 四种食物图像分析方法

1. **Similar Image Search**: This traditional approach calculates similarity between an uploaded food image and images in a database, using models like **VGG-16** and **ResNet-50** for feature extraction.

2. **Improved Similar Image Search**: This optimized version pre-processes the database to enhance retrieval speed and accuracy using vector embeddings and similarity measures such as **Cosine Similarity**.

3. **Image to Ingredient to Recipe**: This approach predicts ingredients from the food image first, using models like **Transformers**, then generates a recipe based on detected ingredients.

4. **Inverse Cooking**: This method identifies ingredients in an image and uses them to generate a recipe, utilizing **Transformers with attention mechanisms** to integrate ingredient and image information for better recipe generation.

---

## 逆向烹饪（Inverse Cooking）基本原理

**Inverse Cooking** uses a **Transformer** architecture to generate recipes by capturing multimodal relationships between images and ingredients, enhancing recipe generation accuracy. This method primarily includes two phases:

### 图像与成分编码 (Image and Ingredient Encoding)

1. The model pre-trains an **image encoder** and **ingredient decoder** to extract visual features from the input image and predict the ingredients.
2. The encoder uses a convolutional neural network (e.g., **ResNet**) to encode the image into high-dimensional vector representations, capturing the visual characteristics of the food.
3. Simultaneously, the ingredient decoder encodes the ingredients into vectors, forming the basis for recipe generation.

### 基于 Transformer 的配方生成 (Recipe Generation with Transformer)

1. In the recipe generation phase, the model uses a **Transformer** instead of a traditional RNN. Each Transformer block consists of two parts: **Self-Attention Layer** and **Conditional Attention Layer**.
2. **Self-Attention Layer**: This is the classic self-attention mechanism in the Transformer, used to capture contextual information during recipe generation, i.e., the relationship between recipe steps.
3. **Conditional Attention Layer**: This layer is adapted to handle **multimodal attention**, combining image features and ingredient embedding information. Specifically, after self-attention, this layer incorporates embeddings from the image and ingredients to refine recipe generation, ensuring consistency between the generated steps and ingredients.
4. During experiments, researchers tried various attention fusion structures (e.g., parallel, additive), and found that directly **concatenating** image features and ingredient embeddings yielded the best results.

Through this multimodal Transformer architecture, the **Inverse Cooking** method can accurately generate recipe steps based on the image, significantly improving the diversity and accuracy of the generated recipes.
