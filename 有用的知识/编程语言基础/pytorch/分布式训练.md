### 1. 单机多卡训练

每个GPU之间需要通信、同步（synchronize），所以并不是有n个GPU，训练速度就能翻n倍的。

例如：

![img](https://cdn.mathpix.com/snip/images/6UpUK2ib1ACVqIj-N7a42p-Gs-V2xUZrTcFTkAWHbD8.original.fullsize.png)

fully connect的情况下，GPU之间可以直接通信；Ring的情况下需要经过Switch，因此Switch这里会成为通信瓶颈。



 单机多卡和多机多卡的区别：

- 单机多卡是"scale-up" (single node, multiple and faster GPUs, make one machine more powerful, but expensive)
- 多机多卡是"scale-out"(multiple nodes distributed training, use multiple in-expensive machines)



## 分布式训练（多机多卡）

> multiple nodes in multiple machines (GPUs): parallelism by distributing your job across multiple machines

两种parallelism：

- **Data Parallelism**: partition the data set across multiple machines and then each one of them can do the training using that dataset. (数据太多，一个节点存不下来，那么就把数据存到不同的节点上。每个节点上的模型都是整个模型。)
- **Model Parallelism:** train portions of a big model on different machines (模型太大，一个节点存不下，那么就把不同的参数存到不同节点上。)
- Hybrid: partition the model and also partition the data.

### 1. Data Parallelism

在每个节点上，我们只用local data来训练模型（每个节点上的模型都是一样的），得到了各自的loss、梯度。但是现在，每个节点上的模型并不能够看到其他节点上的数据。那么，怎么能够更新总体的模型呢？

显然，直接把所有模型求平均一定不是一个好的方法。实际上，应该求平均的是每个节点的算出来的**梯度**。

求梯度的聚合方法也有两种： Centralized和decentralized

- 中心化聚合: 所有节点都把梯度传给中心的parameter server，由ps对所有梯度求平均，然后分配给不同的节点；不同的节点更新模型参数。现在，一个step就已经完成了。
- 去中心化聚合: P2P, all reduce——直接让workers之间交流实现aggregation

这个过程就叫做"synchronize",意为不同节点上模型的**同步**：

> at any instance of time the model at each of the workers is **same** (always working on the same model)

![img](https://jace-yang.github.io/Full-Stack_Data-Analyst/images/DL_GPU_5.png)



### 2. Model Parallelism

Splitting the model across multiple learners，比如下面这个5层神经网络分给4个learner（learner之间连接加粗）

![img](https://jace-yang.github.io/Full-Stack_Data-Analyst/images/DL_GPU_3.png)

因为我们不希望进行太多的跨节点通信，而是希望尽量使用本地的参数，所以要尽量避免跨节点（图中粗线）的参数。



### 3. 中心化聚合的问题和解决

当然了，中心化聚合也有着各种问题：

- 模型太大，则ps存不下。这种情况就只能把模型再分成shards，每个ps只负责一部分模型参数的聚合。
- 当模型数量很多的时候，它们都需要和ps通信，那么ps会变成瓶颈。这当然也是所有中心化仲裁的缺点。
- ps需要等待最慢的节点把梯度算完。在最慢的节点算完之前，那些早已算好的节点属于空闲状态，造成了资源浪费。-- 不过好在，这个问题可以解决：
  - 解决方案1——Synchronous SGD 

![img](https://jace-yang.github.io/Full-Stack_Data-Analyst/images/DL_GPU_6.png)

​	最左侧的Fully Sync-SGD就是不做任何优化的结果。每次ps都需要等待最慢的那个节点算完，其他节点处于空闲状态。

​	中间的K-sync SGD：等待K个节点算完，然后立刻算梯度更新。那些没算完的节点直接cancel掉，不用继续算了。

​	最右侧的K-batch-sync SGD：等待K个**minibatch**算完。例如图中L2在第一个batch算完之后，接着使用当前的模型参数（**没有更新的**）再训第二个minibatch。没算完的节点直接cancel。这个方法是速度最快的，且收敛速度和K-sync SGD一样（因为这两个都训了相同的minibatch个数）。



- - 解决方案2——Asynchronous SGD （异步）

![img](https://jace-yang.github.io/Full-Stack_Data-Analyst/images/DL_GPU_7.png)

​	“异步”的概念：每个节点的模型不一定都是一样的。每次**只有对ps更新有贡献的节点，其参数才会更新**；其余参数不更新，但也不取消，还继续训练。这样，每次参数更新之后，不同节点上的模型版本就不一样了。那些没有对ps更新有贡献的节点还使用着比较老的参数版本，这就是stale version问题。例如最左侧L3的参数更新还是在最老的版本（蓝色）上进行的，然而此时L2都已经更新过一次了（绿色）。



## ALL-reduce

![img](https://github.com/Jace-Yang/Full-Stack_Data-Analyst/raw/main/book/images/DL_GPU_20.png)

- 之前的办法是parameters server（左）
- 现在用新的——logical ring
  - Each node has a left neighbor and a right neighbor
  - Node only sends data to its right neighbor, and only receives data from its left neighbor
- Both PS and Ring all-reduce involve synchronous parameter updates

![img](https://github.com/Jace-Yang/Full-Stack_Data-Analyst/raw/main/book/images/DL_GPU_21.png)

![img](https://github.com/Jace-Yang/Full-Stack_Data-Analyst/raw/main/book/images/DL_GPU_22.png)

