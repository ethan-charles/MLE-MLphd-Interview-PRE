# 推荐中的特征工程

### 1. 原始特征

- 用户侧无非就是人口属性（性别、年龄、职业、位置等）、用户安装的app列表、用户的各种观看+互动历史那一堆；
- 物料侧无非就是基本属性（作者、长度、语言），还有基于内容理解（thanks to nlp and cv）技术打上的一堆一/二级分类、标签、关键词等

**再次批判“深度学习使特征工程过时”的论调**：

比如，用了阿里的DIxN家族（DIN/DIEN/DSIN）能够捕捉用户的短期兴趣，用了SIM能够捕捉用户的长期兴趣，那么未来是不是没必要再对用户历史做特征工程了，把一堆用户观看+互动过的**item id**扔进DIxN+SIM不就行了，省时省力，效果也好？我的答案是否定的，原因有二：

- 2021年留给我最深印象的就是DCN作者的那一句话“People generally consider DNNs as universal function approximators, that could potentially learn all kinds of feature interactions. However, recent studies found that DNNs are inefficient to even approximately model 2nd or 3rd-order feature crosses.”。DNN的“火力”没那么强，**原材料**一古脑扔进去（不加工），有时熟不熟都会成问题，更何况各种滋味的融会贯通。
- 各种强大的网络结构好是好，但并不是无代价的。SIM中用target item去检索用户的长期历史再做attention，都是线上inference的耗时大头。做做候选item只有几百的精排倒还可以，但是让候选item有成千上万的粗排和召回，情何以堪？难道不用SIM就不能刻画用户的长期兴趣了吗？当然不是，接下来要介绍的几个特征工程上的技巧就能够派上用场，**将计算压力从线上转移到线下，离线挖掘用户长期兴趣，使召回+粗排环节也能够用得上**。

接下来，就介绍推荐系统特征工程中的几个高级技巧，将原始特征**加工**成“**让模型更好吸收**”的形状。

### 2. 用户特征

无疑，描述用户兴趣最有用的信息就是各种用户历史。如上所述，除了将用户观看+交互过的**item id**直接一古脑地扔进DIN/DIEN/SIM，让模型自己学出用户各种长短期历史，我们还可以从这些用户历史中**手工挖掘**出一些有用的信号。

**6个维度挖掘用户的长短期兴趣**：

- 用户粒度

- - 单个用户
  - 某一群用户：比如相同年龄段、同性别、同地域、安装了同款app的人群。**基于人群的统计，对于新用户冷启动意义重大**（我们可以根据用户的基础属性特征，去参考属性相同的人群）。

- 时间粒度

- - 最近、过去x小时、过去1天、过去1周、过去1月、从用户首次使用app至今、...**（长短期兴趣相结合）**
  - 太长的时间粒度（e.g.,首次使用至今）在统计的时候，会考虑时间衰减
  - 长期历史的统计，会通过**离线批量任务**（hadoop/spark）的形式完成
  - 短期历史的统计，会直接访问**线上缓存**（redis）

> redis是一个**key-value 存储系统** 。它支持存储的value类型相对更多，包括string (字符串)、list (链表)、set (集合)、zset (sorted set --有序集合)和hash（哈希类型） 简单来说，redis就是把数据库存储在**内存**里，然后隔一断时间再从内存备份到硬盘以防止数据丢失；由于访问内存，所以访问速度特别快，一般用来做缓存。

- 物料粒度

- - 可以是item id
  - 也可以是item上的属性，比如一二级分类、标签、关键词、...

- 动作类型

- - 正向：点击、有效观看、完整观看、点赞、转发、评论、...
  - 负向：忽略（曝光未点击）、短播、踩、...

- 统计对象

- - 次数、时长、金额、...（表达user对item的喜爱程度）

- 统计方法

- - 收集list、计算XTR、计算占比、...

通过以上6个维度的交叉，我们可以构造出一系列特征来描述用户的长期（e.g., 首次使用app至今）、短期（e.g. 过去1天）、超短期（e.g., 刚刚观看的x个视频，session）的兴趣。比如：

- 在过去1天，对tag="坦克"的CTR（i.e., 系统在过去1天，给该用户推了10篇带tag="坦克"的文章，该用户点击了6篇，ctr=0.6）
- 在过去1天，对tag=“坦克”的点击占比（i.e., 在过去1天，用户一共点击了10文章，其中6篇带tag="坦克"，点击占比=0.6）
- 在过去1天，对tag=“坦克”的时长占比（i.e., 在过去1天，用户一共播放了100分钟，其中60分钟消费在带tag="坦克"的物料上，时长占比=0.6）
- 在最近1小时，点击的文章列表
- 在过去1天，忽略（隐式负反馈）category=“时尚”的item个数
- 自首次使用app至今，对tag='坦克'的CTR（统计时，曝光数与点击数都要经过时间衰减）
- 男性用户，在过去1月，对tag="坦克"的文章的CTR

注意：

- 以上6个维度只是为我们手工挖掘用户兴趣提供了一个框架，使我们添加特征时更有章法。至于具体要离线挖掘哪些特征，也要根据算力和收益，综合考虑；
- 这些手工挖掘的用户兴趣信号，可以作为DIN/DIEN/SIM挖掘出来的用户兴趣的**补充**。而在**召回/粗排**这种计算压力大的环节，由于**可以离线挖掘而节省线上耗时**，以上这些手工挖掘出的用户长短期兴趣可以**（局部）代替**DIxN/SIM这些“强但重”的复杂模型。

### 3. 物料特征

对于item侧，我认为最重要的特征就是这些item的**后验统计数据**，

- 时间粒度：全部历史（带衰减）、过去1天，过去6小时，过去1小时、......
- 统计对象：CTR、平均播放进度、平均消费时长、......

比如：某文章在过去6小时的CTR，某文章在过去1天的平均播放时长、......

（回忆快手用户搜索的场景）

但是也要谨记，

- 这些统计数据肯定是**有偏**的，一个item的后验指标好，只能说明推荐系统把它推荐给了对的人，并不意味着把它推给任何人都能取得这么好的效果。

- - 其实这个问题其实也不大，毕竟交给精排模型打分的都已经通过了召回+粗排的筛选，多多少少是和当前用户相关的，之前的统计数据还是有参考意义。

- 利用这些后验统计数据做特征，多少有些纵容马太效应，之前后验数据好的item可能会被排得更靠前，不利于新item的冷启。

- - 那么新item没有后验数据怎么办（新item没有“交叉特征”）？填写成0岂不是太受歧视了？其实有一个办法就是建立一个模型，根据物料的静态信息（e.g., 作者、时长、内容理解打得各种标签等基本稳定不变的信息）**来预测它们的后验数据**。（这也是Airbnb采取的做法，假如我们的预测模型准确率是100%，那么不就没有冷启动问题了！）

------

另外再介绍一个**由用户给物料反向打标签**的trick。

- 一般画像的流程，都是先有物料标签，再将用户消费过的物料的标签积累在用户身上，形成用户画像。
- 反向打标签是指，将消费过这个物料的用户身上的标签积累到这个物料身上。比如：一篇关于某足球明星八卦*绯闻*的文章，由于该明星的名字出现频繁，NLP可能会将其归为“体育新闻”，但是后验数据显示，带“体育”标签的用户不太喜欢这篇文章，反而带“娱乐”标签的用户更喜欢，显然这篇文章也应该被打上“娱乐”的标签。
- 类似的，给物料打上“*小资文青喜欢的top10电影之一*”，或者“*在京日本人光顾最多的日料店*”等，都是由用户消费反向给物料打上的极其重要的标签。

### 4. 交叉特征

说到交叉特征，受“DNN万能论”的影响，近年来已经不再是研究+关注的热点。既然DNN"能够"让喂入的特征“充分”交叉，那何必再费神费力地显式去输入交叉特征呢？

我的答案还是那两条，一来，不要再迷信DNN"万能函数模拟器"的神话；二来，手动交叉的特征犹如加工好的食材，**这么强烈的信号喂到模型的嘴边上**，模型没必要费力咀嚼，**更容易被模型消化吸收**。

比较简单的一种交叉特征就是计算**用户与物料在某个维度上的相似度**。以“标签”维度举例：

- 某用户过去7天**在“标签”维度上的画像**，可以写成一个稀疏向量，`u={‘坦克’:0.8, ‘足球’:0.4, '二战':0.6, '台球':-0.3}`。每个标签后面的分数是由第2节的方法统计出的用户在某个标签上的兴趣强度（可以根据xtr/时长等指标计算出）；
- 某篇文章的标签，可以写成一个稀疏向量，`d={'坦克':1, '二战':0.5, '一战':0.8}`。每个标签后面的分数是**NLP在打这个标签时的置信度**；
- 拿这两个稀疏向量做点积，`u x d={‘坦克’:0.8, ‘足球’:0.4, '二战':0.6, '台球':-0.3} x {'坦克':1, '二战':0.5, '一战':0.8}=0.8*1+0.5*0.6=1.3`，就可以表示这个用户对这篇文章**在“标签”维度上的匹配程度。**

试想，我们如果把这样加工好的、信号如此强的特征加入到模型中，何愁模型表现不好呢？

------

这里介绍一篇显式加入交叉特征的文章：Explicit Semantic Cross Feature Learning via Pre-trained Graph Neural Networks for CTR Prediction（阿里妈妈，SIGIR 21）

文章的Intuition是：我们想要得到**任意一对儿<user特征，item特征>上的消费指标**，比如`<男性用户，item标签=“足球”>`这一特征对上的xtr。之后，我们显式的将这些特征（xtr）加入模型，比起用深度模型希望它得到“特征交叉”，要好得多。要得到这样的特征，离线统计当然是可以的，我们只要数数`<男性用户，item标签=“足球”>`共同出现的样本有多少、其中label = 1（即点击、转化...）的样本又有多少，两者一除就能得到。但是这么做，有两个缺点：

- 数量太多了，我们要统计、存储的特征对为`"所有用户特征 * 所有物料特征"`，要耗费大量的资源，线上检索起来也是个麻烦
- 扩展性太差。只有对共现超过一定次数的特征对上的xtr才置信，才有保留价值。对于共现次数较少，甚至没有共现过的特征对上的xtr，我们也拿不到。

文中提出了用**预训练**来解决以上难题的思路

- 训练一个GNN模型

- - 图的顶点就是样本中出现过的feature，样本中常见共现特征之间建立边，边的权重就是这一对特征离线统计出的**xtr**（或者其他能够表示用户对item喜爱程度的指标），如下图所示：

- ![img](https://pic1.zhimg.com/80/v2-ec4e492141a92e55bcb540f8d180d41c_1440w.png)

- - 按照link prediction（预测边的权重）的方式来训练GNN。用GraphSAGE的方法，得到每个node的embedding，然后把两个node的embedding输入到CrossNet（其实就是个MLP），得到预测的两个节点之间的边权重（xtr）。

- ![img](https://pica.zhimg.com/80/v2-c1c080083920c144b654f003cd996f86_1440w.png)

- 在做loss的时候，有一个小trick，就是做weighted loss。如果我们把每对特征都看作是等权重的，那么一些不常出现的特征和经常出现的特征的loss是同等权重，这显然是不合理的。所以，我们按照每对特征共现的频次进行加权：

- ![img](https://pic2.zhimg.com/80/v2-1de5644a016d35f0106798e0ff0b6eca_1440w.png)

- 其中，$p_{u,v}$是预测的边权重，$ a_{u,v}$是ground truth边权重。前面的log(count(u,v)+t)是loss权重，随着特征u、特征v的共现频次增加而增加。log是为了”饱和化“处理，共现频次太大的时候，也不至于权重过大；加t是为了避免权重为0的情况。

- 线上预测时，将每个用户特征与每个物料特征两两组合，每对特征喂入训练好的GNN模型，得到最后一层（K）的embedding，输入到Crossnet中，得到预估xtr，作为特征，加入到CTR预估模型中去。这里的GNN可以是预训练模型，也可以finetune

![img](https://pic2.zhimg.com/80/v2-66dea5e7832ee29f8f436d41bd203d9e_1440w.png)

- 这种作法，即节省了存储+检索海量特征组合的开销，又因为GNN本身也是基于embedding的，对于罕见或少见特征对的扩展性也比较好。



此篇文章的重点是“用预估代替统计+存储”的思路，是否必须用GNN模型，我看倒也未必。如果担心GNN线上预估耗时问题，我看用FM模型也不错：

- **训练一个常规的FM模型预估xtr**
- 模型训练好后，将每个categorical feature的embedding存储起来
- 线下训练或线上预估时，将所有用户特征与所有物料特征，两两组合。
- 针对每一对儿特征，获取各自的feature embedding，点积再sigmoid，就得到这一对儿特征上的预估xtr，供训练或预测

# 后记

多多少少受“DNN万能函数模拟器”、“DNN能让特征充分交叉”的神话影响，业界对特征工程的研究，远不如模型结构那么热闹。

本文希望传递这样一种思想，即便在DNN时代，特征工程仍然值得我们投入精力，深入研究。相比于粗犷地将原始特征一古脑地扔进DIN/DIEN/SIM这些“强但重”的模型，在原始特征上施以精细的刀工，挖掘出更加直白的信息喂到模型嘴边上，

- 一来，令模型对特征中的信息**“吸收”更好**
- 二来，离线挖掘出强有力的特征，某种程度上能够**取代**那些“强但重”的模型结构，**节省预测耗时**，适用于召回+粗排这些计算紧张的场景