# 推荐系统的评价指标

#### 1. 推荐系统不同维度的指标

###### 1.1 准确率

评估准确率有三种不同的范式。

第一种是将推荐算法看成回归问题，预测user对item的评分。指标有RMSE，MAE

第二种是将推荐算法看成分类问题，预测click/not click。指标有Precision, Recall, F1, AUC.

第三种将推荐算法看成排序问题（learning-to-rank）。指标有MAP，NDCG，MRR

###### 1.2 惊喜度/新颖度/多样性/覆盖率（挖掘长尾的能力）

**覆盖率**(Coverage)的具体计算公式如下：

![img](https://pic4.zhimg.com/80/v2-448c949c061b0ad0ce95664499b18baf_1440w.jpg)

其中分母是所有的物品集合；分子是给所有用户推荐的商品集合。

**多样性：**

用户的兴趣往往是多样的，并且有些产品面对的用户也不止一个(比如智能电视前可能是一家人看电视)，同时人在不同的时间段可能兴趣也不一样(早上看新闻，晚上看电视剧)，个人兴趣也会受心情、天气、节日等多种因素影响。所以我们在给用户做推荐时需要尽量推荐多样的item，让用户从中找到自己感兴趣的。种类更多样的话，总有一款能够击中用户的兴趣点。

在具体推荐系统工程实现中，可以通过对item聚类(可以用机器学习聚类或者根据标签等规则来分类)，在推荐列表中插入不同类别的item的方式来增加推荐系统推荐结果的多样性（重排阶段）。

###### 1.3 流畅度/响应及时性/稳定性/高并发

**实时性**指标：

用户的兴趣是随着时间变化的，推荐需要尽快反应用户兴趣变化。推荐的实时性分为四个等级：天级、小时级、分钟级、秒级。越是响应时间短的对整个推荐系统维护要求越高。可维护性、可拓展性、模型是否可并行训练、需要的计算存储资源、业务落地开发效率。

对于“侵占”用户碎片化时间的产品，如今日头条、快手等，这些产品用户“消耗”item的时间很短，因而建议推荐算法做到分钟级响应用户兴趣变化；对于电影推荐、书推荐等用户需要消耗较长时间“消费”item的产品，可以采用小时级或者天级策略。

**响应及时性**：

推荐接口可以在用户请求推荐服务时及时提供数据反馈, 当然是响应时间越短越好，一般响应时间要控制在200ms之内，超过这个时间人肉眼就可以感受到慢了。

服务器响应会受到很多因素影响，比如网络、Web服务器、操作系统、数据库、硬件等，一般无法保证用户的每次请求都控制在一定时间内。我们一般**采用百分之多少的请求控制在什么时间内**这样的指标来评估接口的响应时间(比如99%的请求控制在50ms之内，即99线为50ms)。

**抗高并发能力指标**：

当用户规模很大时，或者在特定时间点有大量用户访问(比如双十一)时，在同一时间点有大量用户调用推荐服务，推荐接口的压力会很大，推荐系统能否抗住高并发的压力是一个很大的挑战。

我们可以在接口上线前对接口做**压测**，事先了解接口的抗并发能力。另外可以采用一些技术手段来避免对接口的高并发访问，比如增加缓存，web服务器具备横向拓展的能力，在特殊情况下对推荐服务进行分流、限流、降级等。



#### 2. 线上评估

用户通过使用推荐算法产生行为(购买、点击、播放等)，我们通过收集分析用户行为日志来评估相关的指标。像离线评估中所介绍的一些准确度指标(如准确率、召回率等)其实可以通过适当的日志打点来真实的统计出来.

推荐模型上线提供推荐服务后, 最重要的用户行为指标有转化率、购买率、点击率、人均停留时长、人均阅读次数等。线上评估一般会结合AB测试技术，当采用新算法或者有新的UI交互优化时，将用户分为AB两组，先放一部分流量给测试组(有算法或UI优化的组)，对比组是优化之前的组。如果测试组与对比组在相同指标上有更好的表现, 显著(具备统计显著性)提升了点击或者转化，并且提升是稳定的， 后续逐步将优化拓展到所有用户。这种借助AB测试小心求证的方法，可以避免直接一次性将新模型替换旧模型，但是上线后效果不好的情况发生

推荐模型更新流程：

![img](https://pic4.zhimg.com/80/v2-2aa975e42f5916adcdd7b07e036d0273_1440w.jpg)





#### 2. 线上指标比线下测试低是怎么回事？

https://zhuanlan.zhihu.com/p/336959267

**“** 离线场景下我们会优化很多离线模型，也会用诸如准召、auc等指标评估模型效果。很多时候我们会遇到，离线auc涨了一些，但是上了ab后发现指标波动或者涨幅不如预期，那么可能是什么原因呢？**”**

##### 2.1. 特征不一致

**这种情况是最常见的。**实际工作中，工程方面一般是工程团队做的，很有可能导致特征口径不统一，或者由于网络等原因特征根本没传过来。

总之，模型上线会受到各种相关变量的干扰，导致线上评估跟离线评估结果不一致。所以有必要引入AB测试减少新算法上线对用户体验的影响。

##### 2.2 数据穿越

拿1-7号数据训练，但是你的测试集是7号，那么结果一定偏高，一定要用8号以后的数据测试。

或者，使用和label强相关的特征导致的数据泄漏。

##### 2.3 离线提升不置信

1. 评估指标不合理
   首先我们要有业务sense，离线设置的指标是否能跟线上真实对应。比如你离线优化ctr模型的auc，那么线上就关注ctr的线上指标，而cvr、留存等都没有关系。
2. 盲目加特征
   工作中我们可能会做大量的特征，然后加进去train模型看效果涨一点点就觉得有效果，但可能这只是波动，线上也大概率没效果。正确做法是看看权重大的特征是否真的合理，权重小的特征应该被删掉（如果这是个合理特征，一定不能删掉）

##### 2.4. 线上降低不置信

1. 数据量不够
   如果你的场景本身数据量就不多，那么波动其实非常大，这种线上指标是不可靠的。我之前实习做的一个场景，由于产品规划问题流量直接给我砍到了1/10，之后我的ctr/cvr就开始疯狂波动，ab的结果实验组前一天+40%，后一天+10%司空见惯。举个极端的例子，实验组对照组都是一万人，开aa实验应该都是10个人购买；实际上实验组11个人购买，对照组9个人购买。其实这种多一个少一个都是波动，但是你计算一下指标，0.0009和(0.0011-0.0009)/0.0009=22.2%