# 对比学习基础

## **1. 对比学习的渊源与谱系**

对比学习最大的技术源泉来自于度量学习（Metric Learning）。度量学习的优化目标就是说：比如我有正例和负例，它要将实体映射到一个空间里面去。它的目标是让正例在空间中近一些，负例在空间中远一些。近些年，由于BERT的影响，很多做图像领域的学者受到这件事的启发，所以就拿度量学习的框架再加上**自监督**学习的思路，来构造对比学习。所以说，可以把对比学习理解为**自监督版本的度量学习**。这种自监督方式体现在哪里呢？具体的区别就是对比学习的正例是根据一些规则**自动构造的**，而不是通过人工标注的数据（也就是有监督的方式），这就是最大的区别。

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPiatBXkeQWuUGBmUQ4xiahXXV2k0MibPoBNhvQX3tPFFDuTP4dTced1K9gCsXv7dISW44SicbkcRwg1ug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

以一个图像为例子，通过自动构造正例或负例，形成图片的两个view,通过encoder把它们编码，即将输入映射到一个投影空间里。对比学习的优化目标是：希望投影空间里两个正例的距离比较近，如果是负例，则希望它们的距离远一些。那么怎么能够达成这个目标（正例比较近，负例比较远）呢？一般来说，是通过定义损失函数（Loss）来达成。一般对比学习系统用的是**InfoNCE**这个Loss，它是对比学习里最常见的一个Loss。你看图中公式展示的这个Loss：通过分子分母就很容易看出来它的优化目标。分子部分强调正例，希望它的距离越近越好，分母部分强调负例，希望和负例距离越远越好。

不同对比学习模型的差别无非三点：

- 怎么构造正负例（原则上正例应该是自动构造出的，也就是自监督的方式构造的。负例怎么构造？一般来说负例好选，通常就是随机选的。）
- Encoder怎么设计
- Loss怎么设计



## 2.什么是好的对比学习系统

我们先来看看，什么是不好的对比学习系统：

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPiatBXkeQWuUGBmUQ4xiahXXVb2vVicZn0S1slEmNonjnYjqhuApwTruzycApic65bcyvTvLXAGvtICNQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

“模型坍塌”，就是说不论你输入的是什么图片，经过映射函数之后，在投影空间里面，所有图像的编码都会坍塌到同一个embedding。容易发生模型坍塌的模型是一个不好的对比学习系统。

那么，怎样是好的对比学习系统呢？

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPiatBXkeQWuUGBmUQ4xiahXXVppnoHZnqmLqicGupbj36HjY3Ns2lZFHcGNshJTImaOnA8tDNHokTe6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（1）Alignment：希望对比学习把相似的正例在投影空间里面有相近的编码。

（2）Uniformity: 当所有实例映射到投影空间之后，我们希望它在投影空间内的分布是尽可能均匀的。因为我们希望，在对应的Embedding包含的信息里，可以更多保留自己个性化的与众不同的信息。举个例子，比如有两张图片，都是关于狗的，但是一张是在草地上跑的黑狗，一张是在水里游泳的白狗。如果在投影成Embedding后，模型能各自保留各自的个性化信息，那么两张图片在投影空间里面是有一定距离的，以此表征两者的不同。而这就代表了分布的均匀性，指的是在投影球面上比较均匀，而不会说因为都是关于狗的图片，而聚到球面的同一个点中去，那就会忽略掉很多个性化的信息。

一个好的对比学习系统，要兼顾这两者。既要考虑Alignment，相似实例在投影空间里距离越近越好。也要考虑Uniformity，也就是不同实例在投影空间里面分布要均匀一些，让实例映射成embedding之后尽可能多的保留自己的个性化信息。

## 3. 经典对比学习模型

#### 3.1 SimCLR

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPiatBXkeQWuUGBmUQ4xiahXXVs5a5gNcI2QalDtYRiac0fTFGagvrPtrUBs31Pugtn9IWOicQ7ablIMicg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 正负例构造：正例是怎么构造的？SimCLR会拿任意一张图片，对这个图片做一些变形，比如说翻转，抠出一部分，颜色变换一下，等等各种可能的变换。这样，每张图片会构造出它对应的两个正例。而负例，Batch内任意其它图片都可作为这张图片的负例。
- Encoder网络结构是双塔模型，第一部分是ResNet，ResNet先抽取图片中的特征，把图片打成embedding，这个Embedding来表征图片的内容。第二部分是个投影结构projector，一般由两到三层MLP连接构成，projector把Embeddding映射到某个投影空间里面，这也是一个embedding。同时，SimCLR上下分支也完全一样，包括**参数也是共享的**。这样做，它就把对应batch的正负例全部映射成对应的embedding了。
- Loss：对正负例做相似度计算（一般会用cosine）。做完相似度计算后，用对比损失函数InfoNCE来驱动，来达成对比学习的目标，也就是刚才讲的，要求在投影空间里面，用InfoNCE推动正例距离拉近一些，负例距离推远一些。

### 3.2 Moco(Batch外负例)

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPiatBXkeQWuUGBmUQ4xiahXXVY77LWr1pDrLLaWIq4FuLYINbDkhwmhnLbalWNZPMm4lVK8saiaSlygA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一个已有结论：负例用的越多，模型效果越好。我们知道SimCLR是在Batch里面随机选的负例。但是在Batch里采负例有现实问题：Batch size不能无限放大，因为batch size增大对计算资源要求比较高，所以总有个限度。现有结论是负例越多越好，但是batch size制约了负例的采样个数，我们希望能采取一些技术手段，能采样大量的负例，但是又不受Batch size大小限制的约束。MocoV2是典型的解决这种矛盾的例子，也就是说，我们如何能够解除batch size的约束，来大幅增加负例的数量。

上图是Moco V2的模型结构图。其实它和SimCLR的结构基本是类似的，只有一点小差异。它也是上下两个双塔结构，网络结构也由两个映射子结构组成，和SimCLR完全一样。下面这个分支结构本身是和上分支的网络结构是完全一样的。

区别在于，Moco维护了一个负例队列。它实际是想解决一个问题，就是负例受batch size大小制约的影响，怎么解决的呢？通过维护的负例队列来解决。也就是说下分支的正例通过下分支打成embedding,然后会把它放入队列里面（入队），在队列待了太久的会让它出队。Moco的负例采集方法和SimCLR一样，是随机抽取负例。但它不是在batch里面取，而是**从负例队列里取**，这样就避免了负例大小受batch size的影响。

