本文参考：[少数派报告：谈推荐场景下的对比学习](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/TsKtIbsOafDV5DHDAVIvhg)

文章链接：[https://arxiv.org/pdf/2007.12865.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2007.12865.pdf)

## 1.对比学习到底是啥

对于最近很火的“对比学习”，许多人把它和我们之前经常用的向量化召回混为一谈。

毕竟，对比学习的思想就是“拉近正样本，推开负样本”，而向量化召回中，我们使用用户点击的item作为正样本，再用in-batch负采样或者随机负采样等采样方法得到不相关的负样本，模型学习的结果就是让user塔的输出embedding和点击item的embedding点积更大、未点击item的embedding点积更小。两个有代表性的损失函数是交叉熵损失和Pair-wise hinge loss。例如：

![img](https://pic3.zhimg.com/80/v2-99b7fbfb3c16ba52a8db5e9f1bc0eaba_1440w.png)

虽然对比学习和向量化召回同时都用了双塔结构、用了负采样、用了同样的损失函数，但是两者不能混为一谈。这两者最主要的区别是，**向量化召回是有监督学习(supervised learning), 而对比学习是自监督学习(self-supervised learning)**。

更具体来说，在向量化召回中，从海量的click log中获得正样本从来不是问题。相反，大家还要用更加严苛的方式定义正样本，防止引入噪声。例如上一篇Facebook的文章([魔法学院的Chilia：[搜索\] KDD'21 Que2Search: Facebook 搜索内容理解](https://zhuanlan.zhihu.com/p/456163030))中提到，一个正样本需要：（1）用户搜索某个query （2）点了某个商品 （3）进去和商家咨询 （4）商家回复了。只有在24h之内发生如上连续事件，才算得上一个正样本。召回中需要加以研究的是如何构建好的**负样本**。

然而，对比学习是自监督学习，解决的是“标注少或无标注”问题。对比学习通过数据增强(**data augmentation**)构建**正样本**。**数据增强，是对比学习的核心**，即研究如何将一条样本经过变化，构建出与其相似的变体作为正样本。

- 在CV领域，数据增强的方式有很多，例如裁剪、翻转、扭曲和旋转都不影响人类对图像语义的理解，因此可以直接作为正样本。
- 在NLP领域，结构高度**离散**的自然语言则很难构造语义一致的正样本，前人采用了一些数据增强方法来构造，比如替换、删除、重排，但这些方法都是离散的操作，很难把控，容易引入负面噪声。SimCSE是一种非常简单、但是效果很好的生成句子正样本的方法，就是把样本过两次预训练好的BERT，用dropout来获得两个不一样的embedding作为正例对。...对，就这。这种通过改变dropout mask生成正样本的方法可以看作是数据增强的最小形式，因为原样本和生成的正样本的语义是完全一致的，只是生成的embedding不同而已。
- 在推荐领域，数据由大量高维稀疏ID组成，特征之间又有相互关联，如何变化才能构建出合适的正样本，是值得研究的问题。

## 2. 对比学习在推荐中的作用

上文说到，对比学习解决的是“标注少或无标注”问题。在推荐中，我们最不缺的就是数据嘛，那么为什么要用对比学习呢？实际上，我们拥有的数据是高度倾斜的(highly-skewed), 一些热门商品得到了非常多的interaction，但是长尾物品的训练数据是十分稀疏的。在推荐中引入针对这些长尾物品/人群的对比学习，**是一种提升泛化能力的正则化(regularization)方法，**也能够起到debias的效果。

## 3. 谷歌：推荐中的自监督对比学习

这篇文章在双塔模型中加入对比学习，作为主任务(预测是否点击)的辅助任务，让模型对【长尾物品】也能学习出高质量embedding（因为引入了更多长尾物品的增强数据），以改善推荐生态。

### 3.1 构建正样本

一种传统的推荐领域数据增强的方法是Random Feature Masking(RFM)方法：对于item ![[公式]](https://www.zhihu.com/equation?tex=x_i) 随机抽取一半的特征得到变体 ![[公式]](https://www.zhihu.com/equation?tex=y_i) ,经过Encoder得到向量 ![[公式]](https://www.zhihu.com/equation?tex=z_i) （这个Encoder实际上就是item tower）; 另一半特征 ![[公式]](https://www.zhihu.com/equation?tex=y_i%27) 经过同样的Encoder得到向量 ![[公式]](https://www.zhihu.com/equation?tex=z_i%27) 。< ![[公式]](https://www.zhihu.com/equation?tex=z_i%2C+z_i%27) >就是正样本对，它们来自同一个item，距离自然越近越好。用同样的方法，对于item ![[公式]](https://www.zhihu.com/equation?tex=x_j) 也得到增强的embedding ![[公式]](https://www.zhihu.com/equation?tex=z_j%2C+z_j%27) 。因为 ![[公式]](https://www.zhihu.com/equation?tex=z_i%2C+z_j) 来自于不同的item，所以它们的距离应该越远越好。损失函数依然是Batch softmax：

![img](https://pic4.zhimg.com/80/v2-9eb15d0c734cbc0b1054225b9524278f_1440w.jpg)

这种随机mask的方法，由于没有考虑到特征之间的相关性，所以存在着问题。例如，相互关联的两个特征，可能分别被拆到了两个变体中。比如"作者国籍"在 ![[公式]](https://www.zhihu.com/equation?tex=y_i) 中，“作品语言”在 ![[公式]](https://www.zhihu.com/equation?tex=y_i%27) 中，这样两个变体的embedding太容易相似了（因为他们包含了全部的信息），达不到锻炼模型的效果。

为了解决这个问题，论文中提出Correlated Feature Masking, 利用了**每两个特征之间的互信息**：

![img](https://pic2.zhimg.com/80/v2-f5dcd6d0e9fec82adc69549ea1d5bb95_1440w.jpg)

特征 ![[公式]](https://www.zhihu.com/equation?tex=v_i%2Cv_j) 越倾向于同时出现，则它们的互信息越大；如果两个field中的特征互信息很大，那么这两个field就是高度相关的。例如“作者国籍”和“作品语言”。其实，这种计算互信息的方法可以理解为对**类别型数据来计算correlation**（对于连续型特征，就直接计算Pearson相关系数就行了）。

计算好了每两个field的相关性，该怎么构建正样本呢？对于每一个batch，都随机选一个field作为种子 ![[公式]](https://www.zhihu.com/equation?tex=f_%7Bseed%7D) , 找到与它最为相关的n个field (n取所有field个数的一半)，得到变体 ![[公式]](https://www.zhihu.com/equation?tex=y_i) ；另一半特征用来生成变体 ![[公式]](https://www.zhihu.com/equation?tex=y_i%27) 。这样，我们使得高度相关的特征都放在了一起。

### 3.2 多任务学习模型

模型有两个任务：有监督的主任务（预测是否点击）；和自监督的对比学习任务。

![img](https://pic1.zhimg.com/80/v2-1b709a20bcf49b310a0373f3adc1039c_1440w.jpg)

其中主任务是来自曝光日志，就是咱们平时用的双塔模型，item都是较为热门的item。而作为辅助任务的对比学习任务为了达到debias和正则化效果，是从整个item池中**均匀采样**的，这样长尾物品和热门物品都有相同的概率出现，长尾物品有了更多的自监督训练数据，所以模型能够得到更好的长尾物品embedding。由于主任务的item tower和对比学习中用的encoder是**共享参数**的，所以对比学习的辅助loss也用来更新item tower，这就起到了正则化的目的。