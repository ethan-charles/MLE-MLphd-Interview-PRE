# 文本摘要经典模型综述

文本摘要可分为**抽取式摘要**和**生成式摘要**：抽取式摘要从源文档中抽取关键句和关键词组成摘要，摘要信息全部来源于原文，比起生成式摘要更通顺。生成式摘要(NLG, Natural Language Generation)根据原文，允许生成新的词语、短语来组成摘要。此外，按照有无监督数据，文本摘要可以分为**有监督摘要**和**无监督摘要**。

### 1. 抽取式摘要

#### 1.1 TextRank

textrank方法将整个句子构建成图，之后使用类似Pagerank的方法得到每个词语的重要程度。一个词与其前面的N个词、以及后面的N个词之间画一条边，边的权重则是共现次数。下图给出了由一个文档构建的图（去掉了停用词并按**词性**做了筛选，只保留名词、动词、形容词等）：

![img](https://pic2.zhimg.com/80/v2-45e33818add08141d4deea618d1bd6c9_1440w.jpg)

TextRank的迭代计算公式如下：

![img](https://pic2.zhimg.com/80/v2-04b2c330a7f471e8b93f78963e6c1619_1440w.jpg)

可以看出，该公式仅仅比PageRank多了一个权重项Wji，用来表示两个节点之间的边的权重。如此迭代传播各节点的权重，直至收敛。对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词；若这T个单词形成相邻词组，则**合成**多词关键词。

其实，TextRank的方法本质上和**TF-IDF**一样，都是基于**词频**的，虽然textrank考虑了词与词之间的关系。这种基于词频的方法就比较有局限，因为关键的词并不一定是出现次数多的。

此外还有用textrank**抽取句子**的方法，即将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边。这样可以选出TextRank值最高的几个节点对应的句子作为摘要。

#### 1.2 基于聚类的方法

将每个句子视为一个点，按照聚类的方式完成摘要。例如 [Unsupervised Text Summarization Using Sentence Embeddings](https://aishwaryap.github.io/ut/NLPProject.pdf) (2016) 提出的方法，首先利用Skip-Thought Vectors来无监督地得到句子embedding。Skip-thought vector 和普通的RNN得到句子embedding不同的一点在于，它认为一个句子与其上下句是存在某种语义联系的，其动机和word2vec中的skip-gram类似，是想通过一个句子**预测出它上下文的句子**，以此来训练得到**句表示**。例如下图中，原始的文本是"I got back home, I could see the cat on the steps, this was strange." 用GRU encoder对当前句子 "I could see the cat on the steps" 建模，然后用两个独立的GRU decoder分别建模前一个句子和后一个句子。

![img](https://pic1.zhimg.com/80/v2-4947721c7b6c87d7362d45ea542db0b0_720w.png)

是skip-thought模型的目标函数:

![img](https://pic2.zhimg.com/80/v2-861e5f843e633196a736d03503b97b79_720w.png)

即给定一个元组 $(s_{i−1},s_i,s_{i+1})$ ，优化的目标是针对以encoder表示为条件的前向和后向语句的对数概率之和。

通过反向传播的训练过程，我们最终会得到一个性能良好的encoder，并且把这个encoder作为一种生成句向量的通用模型，这就是skip-thought的最终效果。

得到一个比较好的句子表示之后，再使用K-means聚类进行句子聚类，得到K个聚类簇。最后从每个类别中，选择距离质心最近的句子，得到N个句子，作为最终的摘要。

#### 1.3 基于神经网络的方法

基于神经网络的抽取式摘要方法比传统的抽取式摘要方法性能明显更高，下面介绍**序列标注**方式和句子排序方式。

##### 1.3.1 序列标注方式

典型工作：`SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents` (2016)

其核心想法是：为原文中的每一个句子打一个二分类标签（0或1），0代表该句不属于摘要，1代表该句属于摘要。最终摘要由所有标签为1的句子构成。

这种方法的关键在于获得句子的表示，即如何将句子编码为一个向量。文章使用双向GRU分别建模词语级别和句子级别的表示：首先用一个双向的GRU建模word-level表示（图中蓝色的部分），然后对每个sentence中的每个词语求平均，得到一个sentence的embedding；之后再使用一个双向的GRU来建模sentence-level表示（图中红色部分）。![图1 SummaRuNNer模型结构](https://p0.meituan.net/travelcube/933c4d3007231506497ab9abeefbe4b2217164.png)



该模型的训练需要**监督数据**，现有数据集往往没有对应的句子级别的标签，可以通过启发式规则进行获取。具体方法为：首先选取原文中与标准摘要计算ROUGE得分最高的一句话加入候选集合，接着继续从原文中进行选择，保证选出的摘要集合ROUGE得分增加，直至无法满足该条件。得到的候选摘要集合对应的句子设为1标签，其余为0标签。



##### 1.3.2 句子排序方式

上文讲的序列标注法在是先对句子打分、再选择 这样的两阶段方法，但是，这种两步分开的方式可能不是最优的。这是因为在每次打分的时候，我们需要考虑到之前已经被选择的那些句子(sentence scoring can be aware of previously selected sentences). 

NeuSUM (Neural Document Summarization by Jointly Learning to Score and Select Sentences) 提出了一种新的打分方式，解决了此前抽取式文本摘要中句子**打分**和句子**选择**这两部分割裂的问题。

本模型的**训练目标**即学到一个score function **g**，该函数目的是计算加入当前句子后生成的摘要能得到的ROUGE F1的收益(gain)。在测试时，模型在每一个 step t 选出打分函数 **g** 最高的句子。

公式中函数 **r** 即表示ROUGE F1, $S_{t-1}$ 表示已经选出的句子集合，$S_t$表示当前句子。在每一个时间步 **t**，模型会贪婪地选出能够得到最大收益（即使函数 **g** 得到最大值的句子），直至达到摘要限制长度。

![img](https://p0.meituan.net/travelcube/6e38a2cc3fe7c2dd1f83ea87ccedb96211319.png)

本文中建模句子的方式和SummaRuNNer一样，都是用GRU建模word-level，然后再建模sentence-level，得到sentence embedding。

由于句子打分需要同时考虑**当前句子的重要性**和**已经选出的句子集**，所以需要用另一个GRU来建模现在已经选出的句子集（下图右半部分）。current hidden state $h_t = GRU(S_{t-1},h_{t-1})$, 然后在GRU之上连接句子打分器sentence scorer，它是双层的MLP，此时句子 $S_i$的得分为：

$\delta (S_i) = W_s tanh (W_qh_t+W_ds_i)$, 

其中$h_t$ 表征当前已经选出的句子集，$s_i$ 表征第i个句子的embedding。

![图2 NeuSUM模型结构](https://p1.meituan.net/travelcube/79c997d960c08158719d930afd279e25169776.png)

NeuSUM模型结构，选出第5和第1个句子。



### 2 生成式摘要

抽取式摘要在语法、句法上有一定的保证，但是也面临了一定的问题，例如：内容选择错误、连贯性差、灵活性差等问题。生成式摘要允许摘要中包含**新的**词语或短语，灵活性较高。随着近几年神经网络模型的发展，Seq2Seq模型被广泛地用于生成式摘要任务，并取得一定的成果。

#### 2.1 Pointer-Generator模型

Get To The Point: Summarization with Pointer-Generator Networks (2017)

仅使用Seq2Seq来完成生成式摘要存在如下问题：①OOV问题；②inaccurate 问题（生成式不可控，会生成一些奇怪的词语）；③重复生成问题。Pointer-Generator在基于注意力机制的Seq2Seq基础上增加了**Copy**和**Coverage**机制，有效地缓解了上述问题。

**Baseline Seq2Seq**的encoder是一个双向的LSTM，输入序列的hidden state为$h_i$ 。decoder是一个单向的LSTM，训练阶段时参考摘要词依次输入，在时间步 t 得到解码状态 $s_t$ 。使用$h_i$和$s_t$得到该时间步原文第 i 个词注意力权重：
                                                                   $$e_i^t=v^Ttanh⁡(W_hh_i+W_ss_t+b_{attn})$$

​                                                                                 $$a_t=softmax⁡(e_t)$$
加权求和得到context vector $h_t^∗$ ：
​                                                                                    $h_t^∗=∑_ia_i^th_i$
将 $s^t$ 和 $h_t^∗$ 拼接在一起，经过两层线性层和softmax得到单词表概率分布 $P_{vocab}$，然后取得概率最大的那个单词即可。训练时的损失就是log loss，在测试时直接取概率最大的那个单词即可。

**Pointer-Generator Networks** 具有Baseline seq2seq的生成能力和Pointer Network的**Copy**能力。如何权衡一个词应该是生成的还是被copy的？原文中引入了一个权重 $P_{gen}$ 。图2是该网络的模型结构：

![图3 Pointer-Generator 模型结构](https://p0.meituan.net/travelcube/6cd00540f6ce16b22e4c362560f77fdc194239.png)

从Baseline seq2seq的模型结构中得到了 $s_t$ 和 $h_t^∗$，和decoder输入 $x_t$ 一起来计算 $P_{gen}$ ：           
                                                $$p_{gen}=σ(w_{h*}^Th_t^∗+w_s^Ts_t+w_x^Tx_t+b_{ptr}) $$
这时，会扩充单词表形成一个更大的单词表--vocab+输入句子中的单词，该时间步的预测词为w的概率为：   

​                                    $P(w)=p_{gen}P_{vocab}(w)+(1−p_{gen})∑_{i:w_i=w}a_i^t$
其中$a_i^t$ 表示的原文档中的词的attention权重。这样，可以有效地避免OOV问题。

原文中最大的亮点就是运用了**Coverage Mechanism**来解决**重复**生成文本的问题。具体实现上，就是将先前时间步的**注意力权重加到一起**得到 coverage vector $c_t$，用先前的注意力权重决策来影响当前注意力权重的决策，这样就避免在此前就已经有很大注意力权重的位置再分配较大权重，从而避免重复生成文本。
具体地，把$c_t$添加到**注意力权重的计算过程**中，$e_i^t=v^Ttanh⁡(W_hh_i+W_ss_t+w_cc_i^t+b_{attn})$



#### 2.2 Leader-Writer模型

Automatic Dialogue Summary Generation for Customer Service(2019)

本文解决的是**对话摘要**的问题，对话摘要面临几个问题：①逻辑性(logical). 在客服对话的摘要中，应该满足一定的顺序性，例如问题描述需要在解决方案之前；②完整性(integral)，即对话中存在的各个要点都应该在摘要中存在；③关键信息正确(correctness)，例如“用户同意”和“用户不同意”虽然只有一字之差，但含义完全相反；④能够很好的生成长文本。

1. 利用辅助要点序列（Auxiliary keypoint sequence）来解决以上这些挑战。要点（key point）是对话摘要中一个句子的主题，例如“问题描述”。总共总结了51个key point，下面的表格中列出来了一部分：

![img](https://pic1.zhimg.com/80/v2-db1bcde9b28682a944b08a92cf1fa979_720w.png)

为了更好的区分“用户认可”和“用户不认可”这种文本相似度高的要点，我们将对立的要点记为**两个不同的要点**。

因此，我们可以将对话摘要生成的问题建模成一个多任务学习问题。首先模型根据对话信息生成keypoint序列；然后再利用对话信息和生成的keypoint序列生成每个keypoint对应的子摘要；最后把这些子摘要拼接起来即可。由于辅助keypoint序列的词典集合小（本文场景下为51），序列长度一般较短（不超过10），因此容易生成准确的keypoint序列；生成每个keypoint的子摘要，其长度也要显著短于完整摘要，可提高摘要的质量，因此解决了长摘要生成难的问题。

##### **Leader-Writer网络**：

![图4 Leader-Writer 模型](https://p0.meituan.net/travelcube/f47b452513d9a4d0fb2ae8fa5525837b235340.jpg)

###### **Hierarchical Transformer Encoder**

左边两个方框(Token-level Encoder, utterance-level encoder)是用层次Transformer来编码对话的。token-level encoder通过Transformer编码每一句中的单词，并聚合得到每句话的表示；utterance-level encoder也是一个Transformer编码器，只不过输入是句子的embedding，同时使用相对位置编码

###### keypoint序列生成网络（Leader-net）

keypoint序列生成器（Leader-net）是一个标准的**Transformer decoder**，以keypoint序列做为监督信息，根据对话的embedding序列解码得到keypoint序列。

###### 子摘要生成网络（Writer-net）

子摘要生成器（Writer-net）则是使用Pointer-generator机制的Transformer解码器。通过pointer-generator机制，子摘要生成器可以copy原始对话中的部分信息，例如数字和电话号码等。需要注意的是，由于模型中采用了层次化encoder对对话进行编码，在Pointer-generator机制中，为了保证能选择到对话中的词，需要考虑层细化的Pointer-generator机制。

同时，采用keypoint序列生成器的解码状态作为子摘要生成器的解码**起始状态**。基于多任务学习的设置，独立了考虑了要点序列生成和子摘要生成的损失。

###### 训练与预测

在训练阶段，Leader-Writer模型利用keypoint序列与对应的子摘要做为监督信息，学习模型参数。在预测阶段，Leader-Writer模型首先根据对话信息生成keypoint序列，然后根据keypoint序列的每个要点的解码状态生成最后的子摘要，最后拼接摘要后，获取最后的摘要。





> 补充：文本生成的评价指标 
>
> ROUGE 是基于Recall的，即看reference中的n-gram有百分之多少被我们的算法找了出来。ROUGE-1表示我们生成的摘要和reference摘要之间的单个词重叠的个数，ROUGE-2表示双元组重叠的个数。但是这样的问题就是，那些很长的摘要ROUGE分数就会很高，这是因为长摘要更有可能去把reference中的词都找出来。那么，一个解决方法使用ROUGE-F1，即综合考虑了recall和precision。
>
> BLEU 是基于Precision的



----

参考资料

https://zhuanlan.zhihu.com/p/85677258

https://zhuanlan.zhihu.com/p/104094333

