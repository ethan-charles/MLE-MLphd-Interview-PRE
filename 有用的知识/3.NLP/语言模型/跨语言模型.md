# 跨语言模型

在著名的科幻电影《银河系漫游指南》里有一种叫**巴别鱼**的神奇生物。将它塞进耳朵里你就能听懂任何语言。多语种语言模型做得事情和巴别鱼很像，人们希望这个模型能用来处理所有的语言。举个例子，目前的SOTA跨语种模型XLM-RoBERTa能够处理104种语言。

![img](https://pic2.zhimg.com/80/v2-10d7b023c46bbd747203b92fe1b22819_1440w.webp)



**数据集**

训练跨语种语言模型会用到两种语料。一种是单语种（monolingual）语料，另一种是**平行（parallel）语料**。所谓平行语料就是源语言与译文“**对齐**”的语料。所谓对齐也有好几种级别，最常见的是**句子级**对齐，也有按词进行对齐的文本。可想而知，平行语料的获取相比于单语种语料要困难许多。如何充分借助单语种语料来提升模型能力是XLM研究的一个重点。

跨语种语言模型的评价一般有两个大方向，一个是其语义理解能力，另一个是文本生成能力。语义理解能力通常借助**XNLI[2]**数据集，它提供了15种语言的平行文本，每种语言7500对的NLI语料。文本生成通常用翻译任务来评估。

### **模型**

#### **Multilingual Bert（mBERT）**

模型来自于这论文**《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》[3]**，你没有看错，就是发表于2018年大名鼎鼎的BERT论文。

2018年11谷歌就放出了支持104种语言的多语种版本预训练模型，规格是BERT base。这个模型的较新版本是uncased版，即没有对输入文本进行规范化。使用WordPiece算法进行tokenization，词典大小是110k。其他的训练方法和普通的BERT一样，采用的是MLM和NSP两个loss，语料是Wikipedia。

#### **XLM**

模型来自于论文**Cross-lingual language model pretraining**，来自于FAIR，发表在NIPS2019。

XLM使用BPE算法进行tokenization，并且词典大小比mBERT更大，达到200k。论文指出Shared sub-word vocabulary对模型性能有很大的影响，在训练BPE算法的过程中他们使用了特殊的采样方式来避免低资源语种被进行字符集切分。

模型训练使用了三种不同的目标函数，在单语种语料上使用非监督的CLM和MLM。MLM就是masked language modeling。CLM全称是Causal Language Modeling，就是GPT的训练方法。在平行语料上使用的目标称为Translation Language Modeling (TLM)。其训练方式如下图所示，是将平行句子拼接后随机mask，希望让模型能借助另一种语言的信息来还原出被遮蔽的词。从图中可以看出模型用language embedding替换了BERT里的type embedding，并且在做TLM任务时position embedding在两个语言间是**对应**的。

![img](https://pic4.zhimg.com/80/v2-049a3c3d03ea9c260f621f69d6fd00c3_1440w.webp)

