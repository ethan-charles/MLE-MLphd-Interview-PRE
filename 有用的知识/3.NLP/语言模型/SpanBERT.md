# SpanBERT

提出了**更好的 Span Mask 方案**，也再次展示了随机遮盖连续一段字要比随机遮盖掉分散字好；通过**加入 Span Boundary Objective (SBO) 训练目标**，增强了 BERT 的性能，特别在一些与 Span 相关的任务，如抽取式问答。

整体模型结构如下：

![img](https://pic3.zhimg.com/80/v2-c68b466c5cffa5da8af684f9e162b93e_1440w.webp)



- Span Masking: 原始bert的mask方法会让本来应该有强相关的一些连在一起的字词，在训练时是割裂开来的。因此我们就会想到，那**能不能遮盖掉这样连在一起的片段**训练呢？当然可以。首先想到的做法，既然现在遮盖子词，那能不能直接**遮盖整个词**，比如说对于 super + man，只要遮盖就两个同时遮盖掉，这便是 Google 放出的 **BERT WWM 模型**所做的。进一步，因为有些实体是几个词组成的，直接将这个实体都遮盖掉。因此百度在 **ERNIE 模型**中，就引入命名实体（Named Entity）外部知识，**遮盖掉实体单元**，进行训练。以上两种做法比原始做法都有些提升。但这两种做法会让人认为，或许必须得引入类似**词边界信息**才能帮助训练。但前不久的 MASS 模型，却表明可能并不需要，**随机遮盖**可能效果也很好，于是就有本篇的 idea：根据**几何分布**，先随机选择一段（span）的**长度**，之后再根据均匀分布随机选择这一段的**起始位置**，最后按照长度遮盖。文中使用几何分布取 p=0.2，最大长度只能是 10，利用此方案获得平均采样长度分布。
- **Span Boundary Objective** 是该论文加入的新训练目标，希望**被遮盖 Span 边界的词向量，能学习到 Span 的内容**。或许作者想通过这个目标，让模型在一些需要 Span 的下游任务取得更好表现，结果表明也正如此。具体做法是，在训练时取 Span 前后边界的两个词，值得指出，这两个词不在 Span 内，然后**用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词**。详细做法是将词向量和位置向量拼接起来，过两层全连接层。SBO 目标的损失，和 BERT 的 **Mased Language Model （MLM）**的损失加起来，一起用于训练模型。

![img](https://pic3.zhimg.com/80/v2-63a2c45f7e894050bdbccea2fbad1dbe_1440w.webp)

加上 SBO 后效果普遍提高，特别是之前的指代消解任务，提升很大。



![img](https://pic4.zhimg.com/80/v2-2229a8e0f916045940f4cf0cd3e2dfaf_1440w.webp)

