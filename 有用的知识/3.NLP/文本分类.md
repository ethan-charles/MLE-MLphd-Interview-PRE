### 1. ACL2020-多任务负监督方式增加CLS表达差异性

[Text Classification with Negative Supervision](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/文本分类/使用负监督的文本分类)；

文本分类中存在这样一种情况，即“语义相似标签不同”问题。例如，"A cold is a legit disease"标签应该是"Neutral", 即没有明显标签；"Oh my god! I caught a cold"的标签应该是"Cold". 这个也是Adversarial sample的思想，即对句子进行一个很小的改变，却能够明显的改变其label。

对于这种现象，模型往往导致两个句子的[CLS]输出向量很接近，之后接一个分类器，分出的类别就是同一个。作者想解决的一个问题就是，想要通过一种方式，让模型知道，这两句话语义是不相似的，就是想要两个句子的[CLS]的输出是区分度大的。文中提出的模型鼓励不同label的句子都有着相异的表征，强迫把这个距离拉开，这也是对比学习的思想。

[![负监督模型架构](https://camo.githubusercontent.com/78cc0876ed2bd373d6802af02c704d33a37cd902ba34763aecae5f3f62b901d4/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d33302d3039333135342e6a7067)](https://camo.githubusercontent.com/78cc0876ed2bd373d6802af02c704d33a37cd902ba34763aecae5f3f62b901d4/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d33302d3039333135342e6a7067)

作者使用的是一个很简单的多任务架构，分为了两个任务：

1. Main Task：主要任务，做常规的文本分类任务
2. Auxiliary Task：辅助任务，输入负样本（不同类别的样本），计算余弦下相似度。

### 2. **缓解标签不独立以及标注错误的问题**

文本分类普遍存在一个问题，即标签可能不是相互独立的，而且一个样本可能对应多个类别。比如："郭麒麟相声说的是真棒啊，综艺是真好看啊，综艺感真实爆棚了"。你说它的类别是【相声】？【综艺】？【娱乐明星】？还有一个问题，就是标注错误的问题，这种情况一般使用标签平滑(label smoothing)。标签平滑让真实标签不那么极端化，给与标签一定的容错概率。但是标签平滑本质上是加了一个噪声，并不是真实反映标签的分布情况。

1. 可以捕捉标签与标签之间的关系
2. 可以捕捉标签和样本之间的关系
3. 在噪声数据集，效果比LS要好



先来看架构图：

[![LCM架构图](https://camo.githubusercontent.com/0ea68b3e4d715cc8aad4c4b679d8a3fdc5f21abcaf67c47ae6c6f32030fef4e4/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d31302d3130353731382e6a7067)](https://camo.githubusercontent.com/0ea68b3e4d715cc8aad4c4b679d8a3fdc5f21abcaf67c47ae6c6f32030fef4e4/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d31302d3130353731382e6a7067)

架构图最核心的部分注意看紫色的Similarity Layer层，这一层主要做的是对经过深度学习模型学到的句子表征(instance representation)和label representation进行相似性度量。

[![SLD](https://camo.githubusercontent.com/6994069d70fc40234e8d2546cd020628a16b5071aa9a866009a2c4a9d2745e3b/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d31302d3130353731372e6a7067)](https://camo.githubusercontent.com/6994069d70fc40234e8d2546cd020628a16b5071aa9a866009a2c4a9d2745e3b/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31322d31302d3130353731372e6a7067)

$f_L$是对labels进行encode，得到每个**label**的表达向量，方便和句子向量$v^{(i)}$做 dot product。$y^{(c)}$就是这个相似度经过一层MLP，再做softmax的结果。$y^{(t)}$是原始的one-hot label, 和$y^{(c)}$以参数$\alpha$相加，得到最终的标签。损失函数使用的是KL散度。

总结一下，这个方法挖掘了**标签之间**的关系和**标签与样本**之间的关系，能够解决有噪声标签（标注错误）和语义界限比较模糊的问题。

### 3. **只使用标签名称就可以文本分类**

Text Classification Using Label Names Only: A Language Model Self-Training Approach

人脑在对一个文本进行分类的时候，并不需要看到任何标注数据，而只是通过一些关于描述分类类别的单词，就可以做出判断。比如我们可以轻易地把一个新闻分为"sports","business","politics". 

##### step1. Category Understanding via Label Name Replacement

首先，我们需要找到与标签名称语义相关性较高的词汇。人们在看到标签名称的时候，会联想到很多与之相关的词汇。所以，我们首先找到每个句子中存在的标签名字，使用Bert预训练模型，用MLM方法找到与之最接近的50个单词。每个句子中含有的标签词汇总，按照频率，找出排在前100个单词，作为当前标签名称(Label Name)的类别category indicative words，如下图所示：

[![label_change](https://camo.githubusercontent.com/539e655250f362cb5945bcd721c684e9a5629262f27f24a77da8ecafd8b7b416/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d32312d3131343832382e6a7067)](https://camo.githubusercontent.com/539e655250f362cb5945bcd721c684e9a5629262f27f24a77da8ecafd8b7b416/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d32312d3131343832382e6a7067)

**step2. word-level classification via masked category prediction**

一个最简单的办法，就是只要当前的句子出现了`category indicative words`中的词汇，我们就认为当前的句子属于相对应的Label。但是这样做是有很大问题的，这是因为每个单词的词汇意义是与语境有关系的，并不是只要出现了某个词就能够精确地把它分为某个label。而且，虽然category indicative words有100个，但是也毕竟是有限的，还会有未被覆盖的词语。

为了缓解这两个问题，作者提出了`Masked Category Prediction (MCP)`任务。简单讲，它分为两个步骤：

1. 针对句子中的每个单词，使用Bert这种预训练模型，用MLM找到与之最近接的前50个相关词汇；然后将这50个相关和每个标签的category indicative words进行比较，当交集超过20个时候，此时这个句子中的这个单词对应的类别就是对应的这个Label
2. 句子经过第一个步骤之后，句子中**的部分单词就有了类别**。那么mask掉这些单词，然后Bert相对应的每一个单词上面接一个分类器对当前单词做类别的分类。如下图：

[![ Masked Category Prediction](https://camo.githubusercontent.com/39c7f2cf05a2f8fc39361784690067a140659d2c976e1170d7147d6cb6a974ed/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d32312d3131343832372e6a7067)](https://camo.githubusercontent.com/39c7f2cf05a2f8fc39361784690067a140659d2c976e1170d7147d6cb6a974ed/68747470733a2f2f70696373666f726461626c6f672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323032302d31312d32312d3131343832372e6a7067)

**step3. self-training on unlabeled corpus for generalization**

经过第二个步骤，当前模型仍然存在问题：

1. 有的句子没有被找到有类别的单词，所以这些没有被训练到
2. 训练到文本分类模型使用的是对应类别单词mask那里的输出，而不是[CLS]。而ClS一般可以看到整个句子的全部信息。

针对这两个问题，作者提出**使用全部的无标签数据，进行自训练**。

