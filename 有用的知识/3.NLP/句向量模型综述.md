# 句向量模型综述

句向量就是对句子使用一定的方法进行向量化，一些方法如下图所示：

![综述](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%8F%A5%E5%90%91%E9%87%8F/images/%E7%BB%BC%E8%BF%B0.png)

#### 1.最简单的Baseline模型

##### 1.1 基于统计的词袋模型

**One-hot 模型**：一个单词如果出现，则为1；不出现，则为为0。形如[1,1,1,0,1],来将句子向量化。

**TF-IDF**： 使用到了单词在句子中出现的词频(TF)和在所有文档中出现的逆频率(IDF)。所以向量中每个元素的值不再是0/1, 而是浮点数。相比于One-hot，增加了单词重要性这个维度的特征，所以效果一般来说比One-hot要好。

##### 1.2 基于词向量的词袋模型

对词向量最简单的操作就是**求平均**获取句子的表征。对于词向量，一般可以使用Word2vec/Fasttext/Glove。后期Bert出现之后，我们也可以使用Bert的最后一层（或者某一层）的输出作为词向量。

简单求均值简单粗暴，优化方法就是使用各种方法进行**加权**求均值，比如我们可以使用TF-IDF对词向量做加权求和获得句子的表征。

词袋模型存在一个最大问题，就是忽略了句子的**语序**问题。

**1.3基于任务**

举个简单的例子，我们使用RNN和CNN做*文本分类*任务，然后使用最后一个时刻或者最后一层（或者其他方式）作为句子的向量。这种方式很好，但是存在的问题就是句子向量的表达**严重依赖down-stream task**。我们用文本分类训练出现的句子向量如果还是用在文本分类任务，效果可能还不错，但是如果用在其他任务（如情感分类）上，可能就一塌糊涂。这是因为我们的模型是依赖于任务的，文本分类模型侧重点和情感分类的侧重点是不同的，导致模型参数也应该是不相同的。

#### 2. 无监督模型

相比上文所说的“基于任务”的方法，无监督模型最大的好处就是不使用标签数据。

##### 2.1 Skip-Thought Vectors 模型

输入为三个连续的句子$(s_{i-1},s_i,s_{i+1})$，然后使用Encoder-Decoder模型（文中使用的是GRU-GRU），输入中间的句子$s_i$，分别生成上一个句子$s_{i-1}$和下一个句子$s_{i+1}$。这个过程非常类似于Word2vec，其实就是Word2vec的“句子版本”。

![img](https://pic1.zhimg.com/80/v2-4947721c7b6c87d7362d45ea542db0b0_1440w.webp)

上图中，当前句 $s_i$ 是：I could see the cat on the steps，上一个句子$s_{i-1}$是：I got back home，下一个句子$s_{i+1}$是：This was strange，其中\<eos\>是句子结束符。

Encoder是为了得到一个句向量，而两个Decoder都是语言模型。目标函数的形式就是语言模型目标函数的形式：

![img](https://pic2.zhimg.com/80/v2-861e5f843e633196a736d03503b97b79_1440w.webp)

其中 $w_{i+1}^t$ 表示下一个句子的解码器预测出的第 t 个词。

通过反向传播的训练过程，我们最终会得到一个性能良好的Encoder，并且把这个Encoder作为一种生成句向量的通用模型，这就是skip-thought的最终效果。

##### 2.2 Quick-Thought Vectors

Quick-Thought Vectors是对Skip-Thought Vectors 的改进，主要是因为Skip-Thought Vectors 太慢了。Skip-Thought Vectors是一个RNN生成任务，很难并行。Quick-Thought Vectors把生成任务改为了分类任务，Decoder从一组句子中选择出正确的上/下一个句子。它可以被解释为对生成问题的近似。



![img](https://pic4.zhimg.com/80/v2-8a907d43c187666395690502ee799233_1440w.webp)

#### 3. 有监督模型

##### 3.1 InferSent

长期以来，有监督式句向量学习被认为提供的embedding质量更低（如上文所述），但是这种假设似乎已经被推翻，部分归功于InferSent结果的公布。

InferSent使用Sentence Natural Language Inference（**NLI**）数据集（包含570k对标有3种类别的句子：neutral，contradictory和entail）来在句子Encoder之上训练分类器。 两个句子都使用相同的编码器进行编码，而分类器则是根据两个句嵌入构建的对表征进行训练：

![img](https://pic2.zhimg.com/80/v2-8861a78cb5fcfc04fe3107d2f4a3b509_1440w.webp)



InferSent 的成功引发了对以下问题的研究：哪种有监督训练任务可以学习句嵌入，以便能更好地泛化到下游任务？

##### 3.2 Universal Sentence Encoder 

论文主要是提出了一个**统一的句子编码框架**。Encoder使用两种模型，一个是Transformer，是为了获取更高的精度，另一个是DAN (Deep Averaging Network）为了获得更快的速度。DAN结构如下：

![img](https://pic2.zhimg.com/80/v2-bb79f8465693d9834e45a616b6a53631_1440w.webp)

两个的训练方式较为类似，都是通过多任务学习，将encoder用在不同的任务上，包括分类、生成等等，以不同的任务来训练一个Encoder，从而实现泛化的目的。

