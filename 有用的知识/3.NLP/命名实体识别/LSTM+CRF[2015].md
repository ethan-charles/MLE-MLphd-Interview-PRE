# LSTM+CRF [2015]

本文是【命名实体识别】系列文章的第二篇。第一篇我们讲了HMM和CRF，那么这一篇我们讲述经典的LSTM+CRF模型，并且每一部分都对其代码实现进行精讲。

本文参考：

https://zhuanlan.zhihu.com/p/97829287

----

#### 1. LSTM+CRF要解决什么问题

为什么使用LSTM+CRF呢？**序列标注**问题本质上是**分类问题**，因为其具有序列特征，所以LSTM就很合适进行序列标注。的确，我们可以直接利用LSTM进行序列标注。

但是这样的做法有一个问题：每个时刻的输出没有考虑上一时刻的**输出**。我们在利用LSTM进行序列建模的时候，只考虑了输入序列的信息，即单词的信息，但是没有考虑到上一个位置**输出的标签信息**。这样会导致一个问题，以“我 喜欢 跑步”为例，LSTM输出“喜欢”的标签是“动词”，而“跑步”的标签可能也是“动词”。但是实际上，这里的“跑步”应该是“名词”，这是因为“动词”后面很少紧接着“动词”，而大概率应该接“名词”。也就是说，这样的方式来使用LSTM，无法对**标签转移关系**进行建模。所以，我们就在LSTM的基础上引入一个**标签转移矩阵**(transition probability matrix)对标签转移关系进行建模，这就和CRF很像了。我们知道，CRF有两类特征函数：

- 一类是针对observation state（单词$x_i$）与hidden state（标签 $y_i$）的**emission probability**

- 一类是针对hidden state之间的转移概率(**transition probability**)。

在LSTM+CRF模型中，前一类特征函数(emission probability)由LSTM的输出替代，后一类特征函数(transition probability)就变成了标签转移矩阵。

#### 2. LSTM + CRF的结构

如下图所示，对于一个输入序列 $X=(x_1,x_2,x_3,x_4)$，经过Embedding后，输入到LSTM中，每个LSTM hidden state经过MLP后得到每个词对应到每个label（这里有5个label）上的分数。这里label的集合包括起始标签S，结束标签E，以及一般标签L1，L2，L3。

LSTM输出如下：

![img](https://pic4.zhimg.com/80/v2-4a4a768bfe423fcebc4b9b78dc11418f_720w.jpg)

下图是标签转移矩阵 $T$ ，我们可以得到上一个时刻的标签为 $y_i$，下一个时刻标签为 $y_{i+1}$ 的得分，即 $T[y_i,y_{i+1}]$

![img](https://pic4.zhimg.com/80/v2-4774fb7b9b5c75b7e07edf5db2f38847_720w.jpg)

一般来说，对于一个序列 $x$ ，它的长度为 $n$ ，有 $m$ 个可能的标签，那么共有 $m^n$ 个可能的标记序列。一个暴力的方法是，利用LSTM+CRF模型计算出**每个标注序列的得分** $score(y)$ ，然后利用softmax进行归一化求出某个标注序列的概率 $p(y|x)=\frac{exp(score(y))}{Z}$ ，选择概率最大的作为标注结果即可。

这样我们就需要关注几个问题：

1.给定输入 $x$ ，如何计算输出序列为 $y$ 的概率？

2.给定训练数据 (x,y) ，如何对LSTM+CRF模型进行训练？

3.对于训练好的LSTM+CRF模型，给定输入 $x$ ，如何求得最可能的结果 $y$ 。

**2.1 第一个问题: 如何计算标记序列为y的概率？**

 概率的计算 $p(y|x)=\frac{exp(score(y))}{∑_yexp(score(y))}$ ，一般我们会对其取对数，然后再求解，即:

​                                               $$logp(y|x)=score(y)−log(∑_yexp(score(y)))$$ 。

可见，只要我们知道了**如何设计打分函数score(y)**, 概率就迎刃而解了。那么，如何设计打分函数呢？

将序列 $x=(x_1,x_2,...,x_n)$ 输入到LSTM中，得到每个词 $x_i$ 对应的标签得分概率分布 $e_i$ 。 LSTM(x) 的结果是一个矩阵，我们称之为发射矩阵 E 。这是因为经过MLP之后输出的结果是对于所有标签的概率分布。对于词 $x_i$ ，经过LSTM输出的结果为 $e_i$ ，是一个m维向量，那么标签 $y_i$ 的得分为 $e_i[y_i]$ 。（ $y_i$ 为int型数值，表示索引）。

而 $y=(y_1,y_2,...,y_n)$ 是一条链，我们还需要去求标签从$ y_{i−1}$ 到 $y_i$ 的转移分数(transition)。设标签转移矩阵为 $T$ ，那么从 $y_{i−1}$ 到 $y_i$ 的转移分数为 $T[i−1][i]$ 。

最后，把emission和transition的分数加起来可得：

​                                                               $$ score(y)=∑_{i=1}^n (e_i) + ∑_{i=2}^n(T[i−1][i]) $$。



好了，现在我们知道了如何去设计这个打分函数score(y). 然而，要想求上面的概率值，还需要计算**所有可能的序列y**的分数之和 $log(∑_y exp(score(y)))$ 。但是我们知道这种情况下共有 $m^n$ 个可能的标记序列，如果列举所有的序列再求总分，是不可行的。我们可以考虑采用**动态规划**的方法，因为动态规划可以“记忆”前面的计算信息，不必在重新计算。

令 $logZ(y_t=i)=log∑_{y_t}exp(score(start \rightarrow yt))$, 表示从开始时刻到 $t$ 时刻时，$y_t$取值$i$时经过的所有序列得分之和。那么，在 t+1 时刻，$y_{t+1}=j$ 的所有序列的得分之和就可以用递推公式计算出来：

$$logZ(y_{t+1}=j)=log∑_{y_t=0}^{m−1}exp(score(start \rightarrow yt)+T[i,j]+E[t+1,j]) \\ ~~~~~~~~~~=log(∑_{y_t=0}^{m−1}exp(score(start \rightarrow y_t)))+(T[i,j]+E[t+1,j])$$



于是，需要我们维护一个 m 维数组 $dp$ ， $dp[i]$ 表示 $t$ 时刻， 所有$y_t=i$ 的标签序列得分指数和的对数 $logZ(y_t=i)$ 。即 $dp=[logZ(y_t=0),logZ(y_t=1),...,logZ(y_t=m)]$

根据t时刻的 $dp$ 得到下一个时刻的数组 $dp'$:

$dp'=[∑(dp+T[:,0]+E[t+1,0]),∑(dp+T[:,1]+E[t+1,1]),...,∑(dp+T[:,m]+E[t+1,m])]$

当得到最后一个时刻 n 的 dp 数组 时，我们只需要用 $logsumexp(dp)$ 就可以得到最终所有路径的分数的指数和的对数，即 $log(∑_yexp(score(y)))$ 。

这样，我们就能最终得到 $logp(y|x)=score(y)−log(∑_yexp(score(y)))$ 。

**2.2 第二个问题，如何训练模型?**

LSTM+CRF所有的参数都是以神经网络的形式定义，使用负对数似然函数作为损失函数，即

$loss=−logp(y|x)$ ， y 为 x 对应的真实标签数据。之后用梯度下降训练模型即可。

**2.3 第三个问题，当模型训练好后，如何求得最好的结果？**

我们要寻找的最优路径即为**得分最高**的路径。下图是一个示例，任意时刻，带有颜色的箭头表示的路径是指向该节点的得分最高的路径。

![img](https://pic2.zhimg.com/80/v2-4782a5ca7340e6fd940e0f11f4e48425_720w.jpg)

在最后一个时刻T=3，我们只需要找出得分最高的路径（某种颜色表示的路径）指向的节点，然后不断回溯即可找到整体的最优路径。

从上面的叙述中，我们可以发现，要想实现这种方法，我们需要**保存**每个节点对应的得分最高的路径及其分数。

输入序列 $ x=(x_1,x_2,...,x_n)$ ，经过LSTM后得到的发射矩阵为 $E∈R^{(n,m)}$ ，标签转移矩阵为 $T∈R^{(m,m)} $。

首先，在初始的 $t=0$ 时刻，各个标签对应的最大路径的得分 $β0∈R^m$ ，即 $E[0,:]$ 。

$t=1$时刻，我们要求各个节点对应的最大得分路径及其得分。

这时我们对 β0 和 T 使用wise加法，这样，我们就得到一个 R(m,m) 的矩阵 M1 ， M1[i,j] 表示t=0的节点 i 的最大得分路径到t=1时刻的节点 j 形成的路径得分。如果我们想求得t=1时刻到节点 j 的最大得分路径，只需要对 M1[:,j] 求最大值。同样的，如果我们想得到t=1时刻的各个节点对应的最大路径得分 β1 ，我们只需要对 M1 的每一列求最大值。求得最大值的同时将最值路径的来源即箭头的尾部节点记下来，便于遍历查找。这样，我们在得到 β1 的同时，也得到了t=1时刻各个节点对应的最大路径在t=0时刻对应的节点向量 P1∈Rm 。

然后，不断重复上面的步骤，直到得到t=n-1时刻的 βn−1 和 Pn−1 。这时 βn−1 对应的是最后时刻的各个节点对应的最大路径得分。我们求得 βn−1 中最大值对应的索引 In−1 。

这个索引表示最优路径的最后一个节点。然后我们将 In−1 带入 Pn−1 ，得到 In−2=Pn−1[In−1] ，表示最优路径倒数第二个节点，然后依次类推，知道得到 I0=P1[I1] ，这时，最优路径为 I=[I0,I1,...,In−1]。



#### 3. 具体应用方法



![img](https://pic3.zhimg.com/80/v2-16458a338f695c6cbe82532af3b84cc6_720w.jpg)

将两层LSTM的输出加一起得到第三层向量 $c$. 如果不使用CRF的话，这里就可以直接接一层全连接与softmax，输出结果了；如果用CRF的话，需要把 *c* 输入到 CRF 层中，经过 CRF 一通专业缜密的计算，它来决定最终的结果。

这里说一下用于表示序列标注结果的 【BIO 标记法和BIOES标记法】。B 就是标记**某个实体词的开始**，I 表示**某个实体词的中间**，E 表示**某个实体词的结束**，S 表示**这个实体词仅包含当前这一个字**。一般实验效果上差别不大，有些时候用 BIOES 可能会有一点优势

![img](https://pic1.zhimg.com/80/v2-d95ef52e02af82bed6740a003c141db8_720w.jpg)

另外，如果在某些场景下不考虑实体类别，只把实体抽取出来即可，那就直接完事了，但是很多场景下需要同时考虑实体**类别**（比如品牌、地点、人名），那么就需要扩展 BIO 的 tag 列表，给每个“实体类型”都分配一个 B 与 I 的**标签**，例如用“B-brand”来代表“实体词的开始，且实体类型为品牌”。当实体类别过多时，BIOES 的标签列表规模可能就爆炸了。

下面这个图中，A表示用纯LSTM（无CRF）的预测结果，B表示LSTM+CRF的预测结果。

![img](https://pic1.zhimg.com/80/v2-694e0210c9672c3565558104fbc7bcc8_720w.jpg)



#### 4. 延申：BERT+CRF

用 BERT 来做，结构上跟上面是一样的，只是把 LSTM 换成 BERT 就 ok 了。

![img](https://pic3.zhimg.com/80/v2-60c96e63016a6aaa69bda23d66580ae6_720w.jpg)

另外，BERT 还有一个至关重要的训练技巧，就是分层学习率。BERT的参数在 fine-tuning 时，学习率一定要调小，特别时后面还接了别的东西时，一定要按两个学习率走，不然 BERT 很容易就步子迈大了。